# ====================================================================
#  Streamlit Cloud ç’°å¢ƒå•é¡Œã¸ã®æœ€çµ‚å¯¾ç­–ã‚³ãƒ¼ãƒ‰
# ====================================================================
import subprocess
import sys
import os
import streamlit as st

# Streamlit Cloudä¸Šã§å®Ÿè¡Œã•ã‚Œã¦ã„ã‚‹å ´åˆã®ã¿ã€ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®å¼·åˆ¶ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆã‚’å®Ÿè¡Œ
# (ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§ã¯ä¸è¦ãªãŸã‚)
if "STREAMLIT_SHARING_MODE" in os.environ:
    try:
        # pipã‚’ä½¿ã£ã¦google-generativeaiãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’æœ€æ–°ç‰ˆã«å¼·åˆ¶çš„ã«ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰
        subprocess.check_call([
            sys.executable, "-m", "pip", "install", "--upgrade", "google-generativeai"
        ])
        # æ›´æ–°ãŒå®Œäº†ã—ãŸã“ã¨ã‚’ãƒˆãƒ¼ã‚¹ãƒˆé€šçŸ¥ã§çŸ¥ã‚‰ã›ã‚‹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        st.toast("âœ… Geminiãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’æœ€æ–°ç‰ˆã«æ›´æ–°ã—ã¾ã—ãŸã€‚")
    except Exception as e:
        # ã‚‚ã—å¤±æ•—ã—ãŸå ´åˆã¯ã€ã‚¨ãƒ©ãƒ¼ã‚’è¡¨ç¤ºã—ã¦ã‚¢ãƒ—ãƒªã‚’åœæ­¢
        st.error(f"ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®å¼·åˆ¶ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        st.stop()
# ====================================================================
#  å¯¾ç­–ã‚³ãƒ¼ãƒ‰ã“ã“ã¾ã§
# ====================================================================

## ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import pandas as pd
import numpy as np
import plotly.graph_objects as go
import google.generativeai as genai
import json
from io import StringIO
from janome.tokenizer import Tokenizer


# --- 0. Gemini APIã‚­ãƒ¼ã®è¨­å®š ---
# Streamlitã®ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆç®¡ç†æ©Ÿèƒ½ã‹ã‚‰APIã‚­ãƒ¼ã‚’å–å¾—
try:
    ## ç’°å¢ƒå¤‰æ•°ã‹ã‚‰APIã‚­ãƒ¼èª­ã¿è¾¼ã¿
    gemini_api_key = os.environ["GEMINI_API_B2BAPP"]

    if gemini_api_key:
        genai.configure(api_key = gemini_api_key)
        ## st.success("OK:API kye")
    else:
        st.error("APIã‚­ãƒ¼ãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        st.stop("APIã‚­ãƒ¼ã‚¨ãƒ©ãƒ¼ã®ãŸã‚å‡¦ç†ã‚’åœæ­¢ã—ã¾ã™ã€‚")
except Exception as e:
    st.error(f"APIã‚­ãƒ¼è¨­å®šã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ:{e}")
    exit()

# --- 1. Janomeã®åˆæœŸåŒ– ---
try:
    janome_tokenizer = Tokenizer()
except Exception as e:
    st.error(f"Janomeã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚¨ãƒ©ãƒ¼: {e}")
    st.stop()


# --- (æ–°è¦è¿½åŠ ) å‹¤æ€ ãƒ‡ãƒ¼ã‚¿åˆ†æãƒ­ã‚¸ãƒƒã‚¯ ---
def evaluate_work_environment(df):
    """å€‹äººã®å‹¤æ€ ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã€æ•´å½¢æ¸ˆã¿çµæœã¨ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™"""
    # 1. ãƒ‡ãƒ¼ã‚¿æº–å‚™
    df['Work start'] = pd.to_datetime(df['Work start'], errors='coerce')
    df['Work end'] = pd.to_datetime(df['Work end'], errors='coerce')
    df['Break start'] = pd.to_datetime(df['Break start'], errors='coerce')
    df['Break end'] = pd.to_datetime(df['Break end'], errors='coerce')
    df.dropna(subset=['Work start', 'Work end'], inplace=True)

    df['Duration'] = df['Work end'] - df['Work start']
    df['Break Duration'] = df['Break end'] - df['Break start']
    df['Break Duration'].fillna(pd.Timedelta(seconds=0), inplace=True)
    df['Real_Duration'] = df['Duration'] - df['Break Duration']

    daily_summary = df.groupby(df['Work start'].dt.date).agg(
        Total_Work_Duration=('Real_Duration', 'sum'),
        Total_Break_Duration=('Break Duration', 'sum')
    ).reset_index()
    daily_summary.columns = ['Date', 'Total Work Duration', 'Total Break Duration']

    # 2. ç”Ÿãƒ‡ãƒ¼ã‚¿ã®è¨ˆç®—
    daily_summary['Daily_Overtime'] = daily_summary['Total Work Duration'] - pd.Timedelta(hours=8)
    daily_summary['Daily_Overtime'] = daily_summary['Daily_Overtime'].apply(lambda x: max(x, pd.Timedelta(0)))
    
    high_daily_overtime_days = daily_summary[daily_summary['Daily_Overtime'] >= pd.Timedelta(hours=5)]
    is_dangerous_by_daily_ot = False
    if not high_daily_overtime_days.empty:
        high_daily_overtime_days = high_daily_overtime_days.copy()
        high_daily_overtime_days['Date'] = pd.to_datetime(high_daily_overtime_days['Date'])
        monthly_high_ot_counts = high_daily_overtime_days.groupby(high_daily_overtime_days['Date'].dt.to_period('M')).size()
        if (monthly_high_ot_counts >= 4).any():
            is_dangerous_by_daily_ot = True

    overtime_days_full = daily_summary[daily_summary['Total Work Duration'] > pd.Timedelta(hours=8)]
    num_overtime_days = len(overtime_days_full)

    daily_summary['Date'] = pd.to_datetime(daily_summary['Date'])
    if daily_summary.empty:
        return {}, {} 
        
    weekly_work_hours = daily_summary.groupby([daily_summary['Date'].dt.isocalendar().year, daily_summary['Date'].dt.isocalendar().week])['Total Work Duration'].sum().reset_index()
    weekly_work_hours.columns = ['Year', 'Week', 'Total Weekly Duration']
    weekly_work_hours['Overtime'] = weekly_work_hours['Total Weekly Duration'] - pd.Timedelta(hours=40)
    weekly_work_hours['Overtime'] = weekly_work_hours['Overtime'].apply(lambda x: max(x, pd.Timedelta(0)))

    def get_week_start_date(row):
        return pd.to_datetime(f'{int(row["Year"])}-W{int(row["Week"])}-1', format='%G-W%V-%u')
    weekly_work_hours['Week_Start_Date'] = weekly_work_hours.apply(get_week_start_date, axis=1)

    average_weekly_overtime = weekly_work_hours['Overtime'].mean()
    avg_overtime_hours = average_weekly_overtime.total_seconds() / 3600 if pd.notna(average_weekly_overtime) else 0

    dangerous_weeks = weekly_work_hours[weekly_work_hours['Overtime'] >= pd.Timedelta(hours=10)]
    is_dangerous_by_weekly_ot = not dangerous_weeks.empty
    is_dangerous = is_dangerous_by_weekly_ot or is_dangerous_by_daily_ot

    inadequate_break_days = overtime_days_full[overtime_days_full['Total Break Duration'] < pd.Timedelta(hours=1)]
    num_inadequate_break_days = len(inadequate_break_days)

    # 3. å‚¾å‘ã¨ç†ç”±ã®åˆ¤å®š
    overtime_trend = "é€šå¸¸"
    trend_reason = "é€±å¹³å‡æ®‹æ¥­æ™‚é–“ãƒ»æ®‹æ¥­æ—¥æ•°ãŒåŸºæº–ã®ç¯„å›²å†…ã§ã™"
    if is_dangerous:
        overtime_trend = "éé‡åŠ´åƒå‚¾å‘ã‚ã‚Š"
        reasons = []
        if is_dangerous_by_weekly_ot:
            reasons.append("é€±æ®‹æ¥­10hè¶…")
        if is_dangerous_by_daily_ot:
            reasons.append("æœˆé–“5hè¶…æ®‹æ¥­ãŒ4å›ä»¥ä¸Š")
        trend_reason = "ã€".join(reasons) + "ã®ãŸã‚"
    elif avg_overtime_hours >= 5:
        overtime_trend = "æ®‹æ¥­å‚¾å‘ã‚ã‚Š"
        trend_reason = "é€±å¹³å‡æ®‹æ¥­æ™‚é–“ãŒ5æ™‚é–“ã‚’è¶…ãˆã¦ã„ã‚‹ãŸã‚"

    # 4. çµæœã®çµ„ã¿ç«‹ã¦
    raw_results = {
        'avg_overtime_hours': avg_overtime_hours,
        'num_overtime_days': num_overtime_days,
        'num_inadequate_break_days': num_inadequate_break_days,
        'is_dangerous': is_dangerous
    }

    display_results = {
        "æ®‹æ¥­è©•ä¾¡": overtime_trend,
        "è©•ä¾¡ç†ç”±": trend_reason,
        "é€±å¹³å‡æ®‹æ¥­æ™‚é–“": f"{avg_overtime_hours:.2f} æ™‚é–“",
        "æ®‹æ¥­æ—¥æ•°": f"{num_overtime_days}æ—¥",
        "ä¼‘æ†©ä¸è¶³ã®æ—¥æ•°(8hè¶…åŠ´åƒ/1hæœªæº€ä¼‘æ†©)": f"{num_inadequate_break_days}æ—¥",
        "æ®‹æ¥­ãŒå¤šã„é€±(é€±æ®‹æ¥­10hè¶…)": "ãªã—",
        "é•·æ™‚é–“æ®‹æ¥­æ—¥(æ®‹æ¥­5hè¶…/1æ—¥)": "ãªã—"
    }
    if is_dangerous:
        if is_dangerous_by_weekly_ot:
            display_results["æ®‹æ¥­ãŒå¤šã„é€±(é€±æ®‹æ¥­10hè¶…)"] = ", ".join(dangerous_weeks['Week_Start_Date'].dt.strftime('%Y/%m/%dé€±').tolist())
        if is_dangerous_by_daily_ot:
            display_results["é•·æ™‚é–“æ®‹æ¥­æ—¥(æ®‹æ¥­5hè¶…/1æ—¥)"] = "æœˆã«4å›ä»¥ä¸Šç™ºç”Ÿ"

    return display_results, raw_results

def generate_chart_data(all_dfs):
    """è¤‡æ•°äººã®å‹¤æ€ ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…¨ä½“ã®é€±æ¬¡æ¨ç§»ãƒãƒ£ãƒ¼ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã™ã‚‹"""
    if not all_dfs:
        return pd.DataFrame()

    all_weekly_data = []
    
    for df in all_dfs:
        # 1. å€‹äººã®DFã”ã¨ã«ãƒ‡ãƒ¼ã‚¿æº–å‚™
        df['Work start'] = pd.to_datetime(df['Work start'], errors='coerce')
        df['Work end'] = pd.to_datetime(df['Work end'], errors='coerce')
        df['Break start'] = pd.to_datetime(df['Break start'], errors='coerce')
        df['Break end'] = pd.to_datetime(df['Break end'], errors='coerce')
        df.dropna(subset=['Work start', 'Work end'], inplace=True)
        
        df['Duration'] = df['Work end'] - df['Work start']
        df['Break Duration'] = df['Break end'] - df['Break start']
        df['Break Duration'].fillna(pd.Timedelta(seconds=0), inplace=True)
        df['Real_Duration'] = df['Duration'] - df['Break Duration']

        daily_summary = df.groupby(df['Work start'].dt.date).agg(
            Total_Work_Duration=('Real_Duration', 'sum')
        ).reset_index()
        daily_summary.columns = ['Date', 'Total Work Duration']
        daily_summary['Date'] = pd.to_datetime(daily_summary['Date'])

        if daily_summary.empty:
            continue

        # 2. å€‹äººã®é€±æ¬¡ã‚µãƒãƒªãƒ¼ä½œæˆ
        weekly_summary = daily_summary.groupby([
            daily_summary['Date'].dt.isocalendar().year,
            daily_summary['Date'].dt.isocalendar().week
        ]).agg(
            Total_Work_Duration=('Total Work Duration', 'sum'),
            OT_Days=('Total Work Duration', lambda x: (x > pd.Timedelta(hours=8)).sum())
        ).reset_index()
        weekly_summary.columns = ['Year', 'Week', 'Total Weekly Duration', 'OT_Count']
        
        weekly_summary['Overtime'] = weekly_summary['Total Weekly Duration'] - pd.Timedelta(hours=40)
        weekly_summary['Overtime'] = weekly_summary['Overtime'].apply(lambda x: max(x, pd.Timedelta(0)))
        
        all_weekly_data.append(weekly_summary)

    if not all_weekly_data:
        return pd.DataFrame()

    # 3. å…¨å“¡ã®é€±æ¬¡ã‚µãƒãƒªãƒ¼ã‚’çµåˆã—ã¦å¹³å‡åŒ–
    combined_weekly = pd.concat(all_weekly_data, ignore_index=True)
    
    project_avg_weekly = combined_weekly.groupby(['Year', 'Week']).agg(
        Overtime=('Overtime', 'mean'),
        OT_Count=('OT_Count', 'mean')
    ).reset_index()

    # 4. ãƒãƒ£ãƒ¼ãƒˆç”¨ã®DataFrameã‚’ä½œæˆ
    def get_week_start_date(row):
        return pd.to_datetime(f'{int(row["Year"])}-W{int(row["Week"])}-1', format='%G-W%V-%u')
    project_avg_weekly['Week_Start_Date'] = project_avg_weekly.apply(get_week_start_date, axis=1)
    
    if project_avg_weekly.empty:
        return pd.DataFrame()

    project_avg_weekly.set_index('Week_Start_Date', inplace=True)
    
    latest_date = project_avg_weekly.index.max()
    six_months_ago = latest_date - pd.DateOffset(months=6)
    
    full_date_range = pd.date_range(start=six_months_ago, end=latest_date, freq='W-MON')
    
    project_avg_weekly = project_avg_weekly.reindex(full_date_range, fill_value=0)
    
    if 'Overtime' in project_avg_weekly.columns:
         project_avg_weekly['Overtime'] = pd.to_timedelta(project_avg_weekly['Overtime'])

    project_avg_weekly = project_avg_weekly.sort_index()

    chart_df = pd.DataFrame({
        'é€±å¹³å‡æ®‹æ¥­æ™‚é–“(h)': (project_avg_weekly['Overtime'].dt.total_seconds() / 3600),
        'é€±å¹³å‡æ®‹æ¥­æ—¥æ•°': project_avg_weekly['OT_Count']
    })
    
    return chart_df


# --- 2. Big5æ€§æ ¼åˆ†æ + ACNç‹¬è‡ªæ€§æ ¼ã®ãƒ­ã‚¸ãƒƒã‚¯ ---
PERSONALITY_WORDS = {
    ## Big5
    'å¤–å‘æ€§': ['ã¿ã‚“ãª', 'æ¥½ã—ã„', 'ãƒ‘ãƒ¼ãƒ†ã‚£ãƒ¼', 'ä¼šã†', 'è©±ã™', 'æœ€é«˜', 'ï¼', 'ï¼ˆç¬‘ï¼‰'],
    'å”èª¿æ€§': ['å”åŠ›', 'ä¸€ç·’', 'æ‰‹ä¼ã†', 'ã‚ã‚ŠãŒã¨ã†', 'ãŠé¡˜ã„ã—ã¾ã™', 'æ„Ÿè¬', 'ç§ãŸã¡'],
    'èª å®Ÿæ€§': ['è¨ˆç”»', 'ç¢ºå®Ÿ', 'ã¹ã', 'é‡è¦', 'è²¬ä»»', 'ã—ã£ã‹ã‚Š', 'ç®¡ç†', 'å ±å‘Š'],
    'ç¥çµŒç—‡å‚¾å‘': ['å¿ƒé…', 'ä¸å®‰', 'å•é¡Œ', 'é›£ã—ã„', 'å¤§å¤‰', 'ãƒªã‚¹ã‚¯', 'ã™ã¿ã¾ã›ã‚“'],
    'é–‹æ”¾æ€§': ['æ–°ã—ã„', 'ã‚¢ã‚¤ãƒ‡ã‚¢', 'é¢ç™½ã„', 'è©¦ã™', 'æƒ³åƒ', 'ã‚ãã‚ã', 'ï¼Ÿ', 'ãªã‚‹ã»ã©'],
    ## ACNã‚ªãƒªã‚¸ãƒŠãƒ«
    'çŸ­æ°—': ['ã¾ã ', 'é…ã„', 'ãŠãã„', 'ã„ãã’', 'æ€¥ã’', 'ã„ãã', 'æ€¥ã', 'ã‚„ã°', 'æ—©ã'],
    'ãƒ‘ãƒ¯ãƒãƒ©': ['ãŠã„ãŠã„', 'ã¾ã˜ã‹ã‚ˆ', 'ã‚„ã‚Œ', 'ãµã–ã‘ã‚‹ãª']
}

## ãƒãƒ£ãƒƒãƒˆã®ãƒ†ã‚­ã‚¹ãƒˆã‚’Janomeã§å˜èªã«åˆ‡ã‚Šåˆ†ã‘
def analyze_personality(text):
    if not isinstance(text, str) or not text.strip():
        return {p: 0 for p in PERSONALITY_WORDS.keys()}
    # â†“ Janomeã®å‡¦ç†ã«ç½®ãæ›ãˆ
    words = [token.surface for token in janome_tokenizer.tokenize(text)]
    scores = {p: 0 for p in PERSONALITY_WORDS.keys()}
    for personality, keywords in PERSONALITY_WORDS.items():
        for word in keywords:
            scores[personality] += words.count(word)
    return scores

## æ–‡ç« ä¸­ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆã—æ€§æ ¼ã‚¿ã‚¤ãƒ—ã”ã¨ã®ã‚¹ã‚³ã‚¢ç®—å‡º
def get_dominant_personality(scores):
    if not scores or sum(scores.values()) == 0:
        return "åˆ†æä¸èƒ½"
    return max(scores, key=scores.get)

## æ€§æ ¼ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—ã™ã‚‹é–¢æ•°
def calculate_cosine_similarity(vec1, vec2):
    vec1 = np.array(vec1)
    vec2 = np.array(vec2)
    norm1 = np.linalg.norm(vec1)
    norm2 = np.linalg.norm(vec2)
    if norm1 == 0 or norm2 == 0:
        return 0.0
    dot_product = np.dot(vec1, vec2)
    similarity = dot_product / (norm1 * norm2)
    return similarity

## æ€§æ ¼ã‚¹ã‚³ã‚¢ã‚’å…ƒã«ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆã‚’ç”Ÿæˆ
def create_multi_user_radar_chart(result_df, my_name):
    labels = list(PERSONALITY_WORDS.keys())
    fig = go.Figure()

    # è‡ªåˆ†ä»¥å¤–ã®ãƒ¡ãƒ³ãƒãƒ¼ã«ä½¿ç”¨ã™ã‚‹è‰²ã®ãƒªã‚¹ãƒˆã‚’å®šç¾©
    other_colors = [
        '#1f77b4',  # é’
        '#2ca02c',  # ç·‘
        '#9467bd',  # ç´«
        '#ff7f0e',  # ã‚ªãƒ¬ãƒ³ã‚¸
        '#8c564b',  # èŒ¶è‰²
        '#e377c2',  # ãƒ”ãƒ³ã‚¯
        '#7f7f7f',  # ã‚°ãƒ¬ãƒ¼
        '#bcbd22',  # ã‚ªãƒªãƒ¼ãƒ–
        '#17becf'   # ã‚·ã‚¢ãƒ³
    ]
    color_index = 0

    score_details_df = result_df['ã‚¹ã‚³ã‚¢è©³ç´°'].apply(pd.Series)
    max_score = score_details_df.max().max() if not score_details_df.empty else 1

    for index, row in result_df.iterrows():
        user_name = row['ãƒ¦ãƒ¼ã‚¶ãƒ¼']
        scores = row['ã‚¹ã‚³ã‚¢è©³ç´°']
        values = list(scores.values())

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼åã«å¿œã˜ã¦è‰²ã‚’æ±ºå®š
        if user_name == my_name:
            line_color = 'red'
        else:
            line_color = other_colors[color_index % len(other_colors)]
            color_index += 1
        
        fig.add_trace(go.Scatterpolar(
            r=values,
            theta=labels,
            fill='toself',
            name=user_name,
            # line=dict(color=...) ã§ç·šã®è‰²ã‚’æŒ‡å®š
            line=dict(color=line_color),
            hovertemplate=f'<b>{user_name}</b><br>%{{theta}}: %{{r}}<extra></extra>'
        ))

    fig.update_layout(
        polar=dict(
            radialaxis=dict(
                visible=True,
                range=[0, max_score if max_score > 0 else 1]
            )),
        showlegend=True,
        title="å…¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ€§æ ¼ã‚¹ã‚³ã‚¢æ¯”è¼ƒ"
    )
    return fig

# --- 3. Gemini APIã‚’ç”¨ã„ãŸé–¢æ•°ç¾¤ ---
def generate_persona_with_retry(target_user_name, target_user_scores, target_user_text, my_scores, max_retries=3):
    """
    Gemini APIã‚’å‘¼ã³å‡ºã—ã¦ãƒšãƒ«ã‚½ãƒŠã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°
    """
    model = genai.GenerativeModel('gemini-2.5-flash-lite')
    default_response = {
        "persona": "AIã«ã‚ˆã‚‹äººæŸ„ã®è§£æã«å¤±æ•—ã—ã¾ã—ãŸã€‚",
        "common_point": "å…±é€šç‚¹ã®è§£æã«å¤±æ•—ã—ã¾ã—ãŸã€‚",
        "communication_point": "ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒã‚¤ãƒ³ãƒˆã®è§£æã«å¤±æ•—ã—ã¾ã—ãŸã€‚"
    }
    prompt = f"""
    ã‚ãªãŸã¯ã€å„ªç§€ãªçµ„ç¹”äººäº‹ã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®æƒ…å ±ã‚’åŸºã«ã€{target_user_name}ã•ã‚“ã®ãƒšãƒ«ã‚½ãƒŠã‚’åˆ†æã—ã¦ãã ã•ã„ã€‚
    # åˆ†æå¯¾è±¡è€…ã®æƒ…å ±
    - æ°å: {target_user_name}
    - æ€§æ ¼ã‚¹ã‚³ã‚¢: {target_user_scores}
    - ãƒãƒ£ãƒƒãƒˆç™ºè¨€ã®æŠœç²‹: {target_user_text[:200]}
    # æ¯”è¼ƒå¯¾è±¡è€…ï¼ˆè‡ªåˆ†ï¼‰ã®æƒ…å ±
    - æ€§æ ¼ã‚¹ã‚³ã‚¢: {my_scores}
    #ã€æœ€é‡è¦ãƒ«ãƒ¼ãƒ«ã€‘
    - **å¿…ãšã€ä»¥ä¸‹ã®ã‚­ãƒ¼ã‚’æŒã¤JSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ"ã ã‘"ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚**
    - **è§£èª¬ã‚„å‰ç½®ãã€```jsonã®ã‚ˆã†ãªè¿½åŠ ã®æ–‡å­—åˆ—ã¯çµ¶å¯¾ã«å«ã‚ãªã„ã§ãã ã•ã„ã€‚**
    {{
    "persona": "ï¼ˆ{target_user_name}ã•ã‚“ã®äººæŸ„ã‚’50æ–‡å­—ç¨‹åº¦ã§èª¬æ˜ï¼‰",
    "common_point": "ï¼ˆè‡ªåˆ†ã¨ã®æ€§æ ¼ã‚¹ã‚³ã‚¢ã®å…±é€šç‚¹ã‚’30æ–‡å­—ç¨‹åº¦ã§èª¬æ˜ï¼‰",
    "communication_point": "ï¼ˆå††æ»‘ãªé–¢ä¿‚ã‚’ç¯‰ããŸã‚ã®ãƒã‚¤ãƒ³ãƒˆã‚’50æ–‡å­—ç¨‹åº¦ã§èª¬æ˜ï¼‰"
    }}
    """
    for attempt in range(max_retries):
        try:
            response = model.generate_content(prompt)
            cleaned_text = response.text.strip().replace("```json", "").replace("```", "").strip()
            persona_data = json.loads(cleaned_text)
            if all(k in persona_data for k in ["persona", "common_point", "communication_point"]):
                return persona_data
        except (json.JSONDecodeError, Exception) as e:
            st.error(f"AIã‹ã‚‰ã®å¿œç­”å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸï¼ˆè©¦è¡Œ {attempt + 1}å›ç›®ï¼‰: {e}")
            if 'response' in locals() and hasattr(response, 'text'):
                st.text_area("AIã‹ã‚‰ã®ç”Ÿã®å¿œç­”:", response.text, height=150)
            if attempt == max_retries - 1:
                return default_response
    return default_response

def generate_team_atmosphere(text, max_retries=3):
    """
    MTGã®ä¼šè©±ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãƒãƒ¼ãƒ ã®é›°å›²æ°—ã‚’åˆ†æã™ã‚‹é–¢æ•°
    """
    model = genai.GenerativeModel('gemini-2.5-flash-lite')
    default_response = {"atmosphere": "åˆ†æå¤±æ•—", "description": "AIã«ã‚ˆã‚‹åˆ†æã«å¤±æ•—ã—ã¾ã—ãŸã€‚", "weather": "éœ§"}
    weather_options = ['å¿«æ™´', 'æ™´ã‚Œ', 'è–„æ›‡ã‚Š', 'æ›‡ã‚Š', 'é›¨', 'é›ª', 'é›·', 'éœ§', 'æš´é¢¨']
    prompt = f"""
    ã‚ãªãŸã¯ã€çµŒé¨“è±Šå¯Œãªçµ„ç¹”é–‹ç™ºã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒŸãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®ä¼šè©±ãƒ†ã‚­ã‚¹ãƒˆå…¨ä½“ã‹ã‚‰ã€ãƒãƒ¼ãƒ å…¨ä½“ã®é›°å›²æ°—ã‚’åˆ†æã—ã¦ãã ã•ã„ã€‚
    # åˆ†æå¯¾è±¡ã®ä¼šè©±ãƒ†ã‚­ã‚¹ãƒˆï¼ˆæŠœç²‹ï¼‰
    {text[:4000]}
    #ã€æœ€é‡è¦ãƒ«ãƒ¼ãƒ«ã€‘
    - **å¿…ãšã€ä»¥ä¸‹ã®ã‚­ãƒ¼ã‚’æŒã¤JSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ"ã ã‘"ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚**
    - **è§£èª¬ã‚„å‰ç½®ãã€```jsonã®ã‚ˆã†ãªè¿½åŠ ã®æ–‡å­—åˆ—ã¯çµ¶å¯¾ã«å«ã‚ãªã„ã§ãã ã•ã„ã€‚**
    - **"weather"ã®å€¤ã¯ã€å¿…ãšä»¥ä¸‹ã®ãƒªã‚¹ãƒˆã‹ã‚‰æœ€ã‚‚é©åˆ‡ã ã¨æ€ã†ã‚‚ã®ã‚’1ã¤ã ã‘é¸ã‚“ã§ãã ã•ã„ã€‚**
      {weather_options}
    {{
      "atmosphere": "ï¼ˆãƒãƒ¼ãƒ ã®é›°å›²æ°—ã‚’15æ–‡å­—ç¨‹åº¦ã§è¡¨ç¾ï¼‰",
      "description": "ï¼ˆãªãœãã®é›°å›²æ°—ã ã¨åˆ¤æ–­ã—ãŸã‹ã€ç†ç”±ã‚’30æ–‡å­—ç¨‹åº¦ã§èª¬æ˜ï¼‰",
      "weather": "ï¼ˆä¸Šè¨˜ã®ãƒªã‚¹ãƒˆã‹ã‚‰é¸æŠã—ãŸå¤©æ°—ï¼‰"
    }}
    """
    for attempt in range(max_retries):
        try:
            response = model.generate_content(prompt)
            cleaned_text = response.text.strip().replace("```json", "").replace("```", "").strip()
            atmosphere_data = json.loads(cleaned_text)
            if all(k in atmosphere_data for k in ["atmosphere", "description", "weather"]):
                return atmosphere_data
        except (json.JSONDecodeError, Exception) as e:
            st.error(f"ãƒãƒ¼ãƒ é›°å›²æ°—ã®AIåˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸï¼ˆè©¦è¡Œ {attempt + 1}å›ç›®ï¼‰: {e}")
            if attempt == max_retries - 1:
                return default_response
    return default_response

def get_weather_icon(weather_str):
    """ å¤©æ°—ã®æ–‡å­—åˆ—ã«å¯¾å¿œã™ã‚‹çµµæ–‡å­—ã‚’è¿”ã™ """
    weather_map = {
        'å¿«æ™´': 'â˜€ï¸', 'æ™´ã‚Œ': 'æ™´ï¸', 'è–„æ›‡ã‚Š': 'ğŸŒ¥ï¸', 'æ›‡ã‚Š': 'â˜ï¸', 'é›¨': 'ğŸŒ§ï¸',
        'é›ª': 'â„ï¸', 'é›·': 'âš¡ï¸', 'éœ§': 'ğŸŒ«ï¸', 'æš´é¢¨': 'ğŸŒªï¸'
    }
    return weather_map.get(weather_str, 'â“')

# --- 3-3. Gemini APIã‚’ç”¨ã„ãŸç·åˆè©•ä¾¡é–¢æ•° ---
def generate_overall_evaluation(atmosphere_result, result_df, my_name, max_retries=3):
    """
    å…¨ã¦ã®åˆ†æçµæœã‚’çµ±åˆã—ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¸ã®å‚åŠ æ¨å¥¨åº¦ã‚’è©•ä¾¡ã™ã‚‹é–¢æ•°
    """
    model = genai.GenerativeModel('gemini-2.5-flash-lite')
    
    # AIã«æ¸¡ã™ãŸã‚ã«ã€ã“ã‚Œã¾ã§ã®åˆ†æçµæœã‚’è¦ç´„ã—ã¾ã™
    team_atmosphere = atmosphere_result.get('atmosphere', 'ä¸æ˜')
    team_weather = atmosphere_result.get('weather', 'ä¸æ˜')
    
    my_data = result_df[result_df['ãƒ¦ãƒ¼ã‚¶ãƒ¼'] == my_name]
    my_dominant_personality = my_data['æœ€ã‚‚å¼·ã„æ€§æ ¼å‚¾å‘'].iloc[0] if not my_data.empty else 'ä¸æ˜'
    
    other_members_data = result_df[result_df['ãƒ¦ãƒ¼ã‚¶ãƒ¼'] != my_name]
    # `mean()`ã§å¹³å‡å€¤ã‚’è¨ˆç®—ã€‚ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã¯0ã¨ã—ã¾ã™ã€‚
    average_match_score = other_members_data['è‡ªåˆ†ã¨ã®æ€§æ ¼ãƒãƒƒãƒåº¦ (%)'].mean() if not other_members_data.empty else 0
    # `value_counts()`ã§æ€§æ ¼ã‚¿ã‚¤ãƒ—ã®äººæ•°ã‚’æ•°ãˆã€`to_dict()`ã§è¾æ›¸ã«å¤‰æ›ã—ã¾ã™
    team_composition = other_members_data['æœ€ã‚‚å¼·ã„æ€§æ ¼å‚¾å‘'].value_counts().to_dict()

    # è§£æå¤±æ•—æ™‚ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå¿œç­”
    default_response = {
        "recommendation": "è‡ªå·±åˆ¤æ–­ã«å§”ã­ã‚‹",
        "reason": "AIã«ã‚ˆã‚‹ç·åˆè©•ä¾¡ã«å¤±æ•—ã—ã¾ã—ãŸã€‚"
    }

    # AIã«é¸ã°ã›ã‚‹é¸æŠè‚¢ã‚’å®šç¾©
    recommendation_options = ["å¼·ãæ¨å¥¨ã™ã‚‹", "æ¨å¥¨ã™ã‚‹", "è‡ªå·±åˆ¤æ–­ã«å§”ã­ã‚‹", "æ¨å¥¨ã—ãªã„"]

    prompt = f"""
    ã‚ãªãŸã¯ã€è¶…ä¸€æµã®çµ„ç¹”äººäº‹ã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆå…¼ã‚­ãƒ£ãƒªã‚¢ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼ã§ã™ã€‚
    ä»¥ä¸‹ã®å¤šè§’çš„ãªåˆ†æçµæœã‚’åŸºã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ï¼ˆ{my_name}ã•ã‚“ï¼‰ãŒã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«å‚åŠ ã™ã¹ãã‹ã©ã†ã‹ã€ç·åˆçš„ãªè©•ä¾¡ã¨ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’ã—ã¦ãã ã•ã„ã€‚

    # åˆ†æãƒ‡ãƒ¼ã‚¿
    1. **ãƒãƒ¼ãƒ å…¨ä½“ã®é›°å›²æ°—**:
       - é›°å›²æ°—: {team_atmosphere} (å¤©æ°—ã§è¨€ã†ã¨ã€Œ{team_weather}ã€)

    2. **ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ãƒãƒ¼ãƒ ãƒ¡ãƒ³ãƒãƒ¼ã®æ€§æ ¼åˆ†æ**:
       - ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æœ€ã‚‚å¼·ã„æ€§æ ¼å‚¾å‘: {my_dominant_personality}
       - ãƒãƒ¼ãƒ ãƒ¡ãƒ³ãƒãƒ¼ã¨ã®å¹³å‡æ€§æ ¼ãƒãƒƒãƒåº¦: {average_match_score:.1f}%
       - ãƒãƒ¼ãƒ ãƒ¡ãƒ³ãƒãƒ¼ã®æ€§æ ¼å‚¾å‘ã®å†…è¨³: {team_composition}

    #ã€æœ€é‡è¦ãƒ«ãƒ¼ãƒ«ã€‘
    - **å¿…ãšã€ä»¥ä¸‹ã®ã‚­ãƒ¼ã‚’æŒã¤JSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ"ã ã‘"ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚**
    - **è§£èª¬ã‚„å‰ç½®ãã€```jsonã®ã‚ˆã†ãªè¿½åŠ ã®æ–‡å­—åˆ—ã¯çµ¶å¯¾ã«å«ã‚ãªã„ã§ãã ã•ã„ã€‚**
    - **"recommendation"ã®å€¤ã¯ã€å¿…ãšä»¥ä¸‹ã®ãƒªã‚¹ãƒˆã‹ã‚‰æœ€ã‚‚é©åˆ‡ã ã¨æ€ã†ã‚‚ã®ã‚’1ã¤ã ã‘é¸ã‚“ã§ãã ã•ã„ã€‚**
      {recommendation_options}
    - **"reason"ã¯ã€ãã®çµè«–ã«è‡³ã£ãŸæ ¹æ‹ ã‚’20æ–‡å­—ç¨‹åº¦ã§ç°¡æ½”ã«è¿°ã¹ã¦ãã ã•ã„ã€‚**

    {{
      "recommendation": "ï¼ˆä¸Šè¨˜ã®ãƒªã‚¹ãƒˆã‹ã‚‰é¸æŠï¼‰",
      "reason": "ï¼ˆè©•ä¾¡ç†ç”±ã‚’20æ–‡å­—ç¨‹åº¦ã§èª¬æ˜ï¼‰"
    }}
    """
    for attempt in range(max_retries):
        try:
            response = model.generate_content(prompt)
            cleaned_text = response.text.strip().replace("```json", "").replace("```", "").strip()
            evaluation_data = json.loads(cleaned_text)
            
            if all(k in evaluation_data for k in ["recommendation", "reason"]):
                return evaluation_data # æˆåŠŸ
        except (json.JSONDecodeError, Exception) as e:
            st.error(f"ç·åˆè©•ä¾¡ã®AIåˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸï¼ˆè©¦è¡Œ {attempt + 1}å›ç›®ï¼‰: {e}")
            if attempt == max_retries - 1:
                return default_response
    
    return default_response

def get_recommendation_color(recommendation_str):
    """ æ¨å¥¨åº¦ã«å¿œã˜ã¦è‰²ã‚’è¿”ã™ """
    if recommendation_str == "å¼·ãæ¨å¥¨ã™ã‚‹":
        return "green"
    elif recommendation_str == "æ¨å¥¨ã™ã‚‹":
        return "blue"
    elif recommendation_str == "æ¨å¥¨ã—ãªã„":
        return "red"
    else: # è‡ªå·±åˆ¤æ–­ã«å§”ã­ã‚‹
        return "orange"

# --- 4. Streamlitã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ç”»é¢ ---
st.title('ğŸ“Š ã‚¢ã‚µã‚¤ãƒ³æ¤œè¨PJã®åˆ†æã‚¢ãƒ—ãƒª')
st.write('ã‚¢ã‚µã‚¤ãƒ³äºˆå®šã®PJãƒ¡ãƒ³ãƒãƒ¼ã®ãƒãƒ£ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã€MTGä¼šè©±ãƒ‡ãƒ¼ã‚¿ã€å‹¤æ€ ãƒ‡ãƒ¼ã‚¿ã¨ã‚ãªãŸã®ãƒãƒ£ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿(CSV)ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã¨ã€ãƒãƒ¼ãƒ ã®é›°å›²æ°—ã‚„åŠ´åƒç’°å¢ƒã€ãƒ¡ãƒ³ãƒãƒ¼ã®æ€§æ ¼å‚¾å‘ã‚’åˆ†æã—ã€PJã¨ã‚ãªãŸã®ãƒãƒƒãƒãƒ³ã‚°ã‚’è¨ºæ–­ã—ã¾ã™ã€‚')
st.write('---')

col1, col2, col3, col4 = st.columns(4)
with col1:
    st.subheader("ğŸ—« PJãƒãƒ£ãƒƒãƒˆ")
    chat_files = st.file_uploader("ãƒãƒ£ãƒƒãƒˆCSVã‚’é¸æŠ", type="csv", accept_multiple_files=True, key="chat_uploader")
with col2:
    st.subheader("ğŸ—£ PJã®MTGä¼šè©±")
    transcript_files = st.file_uploader("éŸ³å£°ãƒ†ã‚­ã‚¹ãƒˆCSVã‚’é¸æŠ", type="csv", accept_multiple_files=True, key="transcript_uploader")
with col3:
    st.subheader("ğŸ—¨ è‡ªåˆ†ã®ãƒãƒ£ãƒƒãƒˆ")
    my_file = st.file_uploader("è‡ªåˆ†ã®ãƒãƒ£ãƒƒãƒˆCSVã‚’é¸æŠ", type="csv", accept_multiple_files=False, key="mychat_uploader")
with col4:
    st.subheader("ğŸ¢ PJã®å‹¤æ€ ãƒ‡ãƒ¼ã‚¿")
    work_files = st.file_uploader("å‹¤æ€ ãƒ‡ãƒ¼ã‚¿CSVã‚’é¸æŠ", type="csv", accept_multiple_files=True, key="work_uploader")
st.write('---')

if (chat_files or transcript_files or work_files) and my_file:
    try:
        # --- ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿å‡¦ç† ---
        team_chat_dfs = []
        if chat_files:
            for file in chat_files:
                file.seek(0)
                try: df_single = pd.read_csv(file, encoding='shift_jis')
                except UnicodeDecodeError:
                    file.seek(0)
                    df_single = pd.read_csv(file, encoding='utf-8')
                team_chat_dfs.append(df_single)
        team_chat_df = pd.concat(team_chat_dfs, ignore_index=True) if team_chat_dfs else pd.DataFrame()

        transcript_text = ""
        if transcript_files:
            team_transcript_dfs = []
            for file in transcript_files:
                file.seek(0)
                try: df_single = pd.read_csv(file, encoding='shift_jis')
                except UnicodeDecodeError:
                    file.seek(0)
                    df_single = pd.read_csv(file, encoding='utf-8')
                team_transcript_dfs.append(df_single)
            if team_transcript_dfs:
                team_transcript_df = pd.concat(team_transcript_dfs, ignore_index=True)
                if 'message' in team_transcript_df.columns:
                    transcript_text = ' '.join(team_transcript_df['message'].fillna('').astype(str))

        # (æ–°è¦è¿½åŠ ) å‹¤æ€ ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
        all_member_work_dfs = []
        if work_files:
            for file in work_files:
                file.seek(0)
                try: df_single = pd.read_csv(file, encoding='shift_jis')
                except UnicodeDecodeError:
                    file.seek(0)
                    df_single = pd.read_csv(file, encoding='utf-8')
                # ãƒ¦ãƒ¼ã‚¶ãƒ¼åã‚’ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰å–å¾—ï¼ˆä»®ï¼‰CSVã«'user'åˆ—ãŒã‚ã‚Œã°ãã¡ã‚‰ã‚’å„ªå…ˆ
                if 'user' not in df_single.columns:
                    df_single['user'] = os.path.splitext(file.name)[0]
                all_member_work_dfs.append(df_single)

        try:
            my_file.seek(0)
            my_df = pd.read_csv(my_file, encoding='shift_jis')
        except UnicodeDecodeError:
            my_file.seek(0)
            my_df = pd.read_csv(my_file, encoding='utf-8')

        if 'user' in my_df.columns and not my_df.empty:
            my_name = my_df['user'].iloc[0]
            st.info(f"ã‚ãªãŸã®ãƒ¦ãƒ¼ã‚¶ãƒ¼åã‚’ã€Œ{my_name}ã€ã¨ã—ã¦èªè­˜ã—ã¾ã—ãŸã€‚")
        else:
            st.error("ã‚ãªãŸã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã« 'user' åˆ—ãŒå­˜åœ¨ã—ãªã„ã‹ã€ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚"); st.stop()
        
        df = pd.concat([team_chat_df, my_df], ignore_index=True)
        st.success(f'{len(chat_files) + len(transcript_files) + len(work_files) + 1}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«æˆåŠŸï¼')

        if st.button('åˆ†æã‚’å®Ÿè¡Œã™ã‚‹'):
            st.write('---'); st.header('åˆ†æçµæœ')
            atmosphere_result, result_df = None, pd.DataFrame() # çµæœã‚’ä¿å­˜ã™ã‚‹å¤‰æ•°ã‚’åˆæœŸåŒ–

            # --- ãƒãƒ¼ãƒ ã®é›°å›²æ°—åˆ†æ ---
            if transcript_text:
                st.subheader('ğŸ¤” PJãƒãƒ¼ãƒ ã®é›°å›²æ°—')
                with st.spinner('AIãŒãƒãƒ¼ãƒ ã®é›°å›²æ°—ã‚’åˆ†æä¸­ã§ã™...'):
                    atmosphere_result = generate_team_atmosphere(transcript_text)
                    weather_str = atmosphere_result.get('weather', 'éœ§')
                    col_atm1, col_atm2, col_atm3 = st.columns(3)
                    col_atm1.metric("ç¾åœ¨ã®ãƒãƒ¼ãƒ ã®å¤©æ°—", weather_str, get_weather_icon(weather_str))
                    col_atm2.info(f"**é›°å›²æ°—**: {atmosphere_result.get('atmosphere', 'N/A')}")
                    col_atm3.success(f"**ç†ç”±**: {atmosphere_result.get('description', 'N/A')}")
                st.write('---')
            
            # --- æ€§æ ¼ãƒãƒƒãƒåº¦åˆ†æ ---
            if not df.empty and 'user' in df.columns and df['user'].nunique() > 1:
                df['message'] = df['message'].fillna('')
                user_texts = df.groupby('user')['message'].agg(' '.join).reset_index()
                results = []
                with st.spinner('æ€§æ ¼ã‚¹ã‚³ã‚¢ã‚’åˆ†æä¸­ã§ã™...'):
                    for _, row in user_texts.iterrows():
                        scores = analyze_personality(row['message'])
                        results.append({
                            'ãƒ¦ãƒ¼ã‚¶ãƒ¼': row['user'], 'æœ€ã‚‚å¼·ã„æ€§æ ¼å‚¾å‘': get_dominant_personality(scores),
                            'ç™ºè¨€æ•°': df[df['user'] == row['user']].shape[0],
                            'ã‚¹ã‚³ã‚¢è©³ç´°': scores, 'å…¨ç™ºè¨€': row['message']
                        })
                result_df = pd.DataFrame(results)
                if my_name in result_df['ãƒ¦ãƒ¼ã‚¶ãƒ¼'].values:
                    my_scores_row = result_df[result_df['ãƒ¦ãƒ¼ã‚¶ãƒ¼'] == my_name].iloc[0]
                    my_scores = my_scores_row['ã‚¹ã‚³ã‚¢è©³ç´°']
                    my_scores_list = list(my_scores.values())
                    match_percentages = [
                        round(calculate_cosine_similarity(my_scores_list, list(row['ã‚¹ã‚³ã‚¢è©³ç´°'].values())) * 100, 2)
                        if row['ãƒ¦ãƒ¼ã‚¶ãƒ¼'] != my_name else 100.0
                        for _, row in result_df.iterrows()
                    ]
                    result_df['è‡ªåˆ†ã¨ã®æ€§æ ¼ãƒãƒƒãƒåº¦ (%)'] = match_percentages
                    st.subheader(f'ğŸ«¡ {my_name} ã¨PJãƒ¡ãƒ³ãƒãƒ¼ã®æ€§æ ¼ãƒãƒƒãƒåº¦')
                    st.dataframe(result_df[['ãƒ¦ãƒ¼ã‚¶ãƒ¼', 'æœ€ã‚‚å¼·ã„æ€§æ ¼å‚¾å‘', 'ç™ºè¨€æ•°', 'è‡ªåˆ†ã¨ã®æ€§æ ¼ãƒãƒƒãƒåº¦ (%)']].sort_values('è‡ªåˆ†ã¨ã®æ€§æ ¼ãƒãƒƒãƒåº¦ (%)', ascending=False))
                    st.write('---')
                    st.subheader('ğŸ“ˆ è‡ªåˆ†ã¨PJãƒ¡ãƒ³ãƒãƒ¼ã®æ€§æ ¼æ¯”è¼ƒãƒãƒ£ãƒ¼ãƒˆ')
                    st.plotly_chart(create_multi_user_radar_chart(result_df, my_name), use_container_width=True)
                    st.write('---')
                    st.subheader('ğŸ­ PJãƒ¡ãƒ³ãƒãƒ¼ã®ãƒšãƒ«ã‚½ãƒŠåˆ†æ')
                    other_users_df = result_df[result_df['ãƒ¦ãƒ¼ã‚¶ãƒ¼'] != my_name].sort_values('è‡ªåˆ†ã¨ã®æ€§æ ¼ãƒãƒƒãƒåº¦ (%)', ascending=False)
                    with st.spinner('AIãŒå„ãƒ¡ãƒ³ãƒãƒ¼ã®ãƒšãƒ«ã‚½ãƒŠã‚’ç”Ÿæˆä¸­ã§ã™...'):
                        user_list = list(other_users_df.iterrows())
                        for i in range(0, len(user_list), 2):
                            col1, col2 = st.columns(2)
                            # 1äººç›®
                            _, row1 = user_list[i]
                            with col1.expander(f"**{row1['ãƒ¦ãƒ¼ã‚¶ãƒ¼']}ã•ã‚“** ã®ãƒšãƒ«ã‚½ãƒŠã‚’è¦‹ã‚‹", expanded=True):
                                persona = generate_persona_with_retry(row1['ãƒ¦ãƒ¼ã‚¶ãƒ¼'], row1['ã‚¹ã‚³ã‚¢è©³ç´°'], row1['å…¨ç™ºè¨€'], my_scores)
                                st.info(f"**äººæŸ„**: {persona.get('persona', 'N/A')}")
                                st.success(f"**å…±é€šç‚¹**: {persona.get('common_point', 'N/A')}")
                                st.warning(f"**ãƒã‚¤ãƒ³ãƒˆ**: {persona.get('communication_point', 'N/A')}")
                            # 2äººç›® (å­˜åœ¨ã™ã‚Œã°)
                            if (i + 1) < len(user_list):
                                _, row2 = user_list[i + 1]
                                with col2.expander(f"**{row2['ãƒ¦ãƒ¼ã‚¶ãƒ¼']}ã•ã‚“** ã®ãƒšãƒ«ã‚½ãƒŠã‚’è¦‹ã‚‹", expanded=True):
                                    persona = generate_persona_with_retry(row2['ãƒ¦ãƒ¼ã‚¶ãƒ¼'], row2['ã‚¹ã‚³ã‚¢è©³ç´°'], row2['å…¨ç™ºè¨€'], my_scores)
                                    st.info(f"**äººæŸ„**: {persona.get('persona', 'N/A')}")
                                    st.success(f"**å…±é€šç‚¹**: {persona.get('common_point', 'N/A')}")
                                    st.warning(f"**ãƒã‚¤ãƒ³ãƒˆ**: {persona.get('communication_point', 'N/A')}")
            else:
                 st.info("æ€§æ ¼åˆ†æã‚’è¡Œã†ã«ã¯ã€PJãƒ¡ãƒ³ãƒãƒ¼ã®ãƒãƒ£ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã¨è‡ªåˆ†ã®ãƒãƒ£ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸¡æ–¹ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")

            # --- (æ–°è¦è¿½åŠ ) å‹¤æ€ ãƒ‡ãƒ¼ã‚¿åˆ†æ ---
            if all_member_work_dfs:
                st.write('---')
                st.subheader('ğŸ¢ PJãƒãƒ¼ãƒ ã®åŠ´åƒç’°å¢ƒ')
                with st.spinner('å‹¤æ€ ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æä¸­ã§ã™...'):
                    individual_results = []
                    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã”ã¨ã«å‹¤æ€ ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆã—ã¦åˆ†æ
                    all_work_df_combined = pd.concat(all_member_work_dfs, ignore_index=True)
                    unique_users = all_work_df_combined['user'].unique()
                    
                    user_dfs_for_chart = []
                    for user in unique_users:
                        user_df = all_work_df_combined[all_work_df_combined['user'] == user].copy()
                        user_dfs_for_chart.append(user_df)
                        display_res, raw_res = evaluate_work_environment(user_df)
                        individual_results.append({
                            'user': user,
                            'display': display_res,
                            'raw': raw_res
                        })

                    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…¨ä½“ã®è©•ä¾¡
                    num_individuals = len(individual_results)
                    avg_weekly_ot = sum(res['raw']['avg_overtime_hours'] for res in individual_results) / num_individuals
                    avg_overtime_days = sum(res['raw']['num_overtime_days'] for res in individual_results) / num_individuals
                    avg_inadequate_break = sum(res['raw']['num_inadequate_break_days'] for res in individual_results) / num_individuals

                    project_overtime_trend = "é€šå¸¸"
                    project_trend_reason = "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…¨ä½“ã§åŸºæº–ã®ç¯„å›²å†…ã§ã™"
                    is_project_dangerous = any(res['raw']['is_dangerous'] for res in individual_results)
                    
                    if is_project_dangerous:
                        project_overtime_trend = "å±é™ºãªåŠ´åƒç’°å¢ƒ"
                        project_trend_reason = "å€‹äººè©•ä¾¡ã«ã€Œéé‡åŠ´åƒå‚¾å‘ã‚ã‚Šã€ã®ãƒ¡ãƒ³ãƒãƒ¼ãŒå«ã¾ã‚Œã¦ã„ã¾ã™"
                    elif avg_weekly_ot >= 5:
                        project_overtime_trend = "æ®‹æ¥­å‚¾å‘ã‚ã‚Š"
                        project_trend_reason = "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®é€±å¹³å‡æ®‹æ¥­æ™‚é–“ãŒ5æ™‚é–“ã‚’è¶…ãˆã¦ã„ã¾ã™"
                    
                    st.info(f"**ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…¨ä½“ã®æ®‹æ¥­è©•ä¾¡: {project_overtime_trend}** ({project_trend_reason})")
                    
                    # ãƒãƒ£ãƒ¼ãƒˆè¡¨ç¤º
                    chart_df = generate_chart_data(user_dfs_for_chart)
                    if not chart_df.empty:
                        st.line_chart(chart_df)

                    # å€‹äººåˆ¥è©•ä¾¡ã®è¡¨ç¤º
                    for res in individual_results:
                        with st.expander(f"**{res['user']}ã•ã‚“** ã®å‹¤æ€ çŠ¶æ³è©³ç´°"):
                            st.table(pd.DataFrame([res['display']]))

            # --- ç·åˆè©•ä¾¡ ---
            st.write('---')
            st.header('ğŸ‘‰ ç·åˆè©•ä¾¡')

            # é›°å›²æ°—åˆ†æã¨æ€§æ ¼åˆ†æã®ä¸¡æ–¹ã®ãƒ‡ãƒ¼ã‚¿ãŒæƒã£ã¦ã„ã‚‹ã‹ç¢ºèª
            if atmosphere_result and not result_df.empty:
                with st.spinner('AIãŒã™ã¹ã¦ã®çµæœã‚’çµ±åˆã—ã€æœ€çµ‚è©•ä¾¡ã‚’ç”Ÿæˆä¸­ã§ã™...'):
                    evaluation = generate_overall_evaluation(atmosphere_result, result_df, my_name)
                    recommendation = evaluation.get('recommendation', 'è©•ä¾¡ä¸èƒ½')
                    reason = evaluation.get('reason', 'ç†ç”±ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚')
                    color = get_recommendation_color(recommendation)
                    
                    st.markdown(f"### ã‚ãªãŸã¸ã®æ¨å¥¨åº¦: <font color='{color}'>**{recommendation}**</font>", unsafe_allow_html=True)
                    st.info(f"**ç†ç”±**: {reason}")
            else:
                st.warning("ç·åˆè©•ä¾¡ã‚’è¡Œã†ã«ã¯ã€PJãƒ¡ãƒ³ãƒãƒ¼ã®ã€ŒMTGä¼šè©±ã€ã¨ã€ŒTeamsãƒãƒ£ãƒƒãƒˆã€ã®ä¸¡æ–¹ã®ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦åˆ†æã‚’å®Ÿè¡Œã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚")

    except Exception as e:
        st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")