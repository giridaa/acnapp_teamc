## ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import streamlit as st
import pandas as pd
import numpy as np
import plotly.graph_objects as go
import google.generativeai as genai
import os
import json
from io import StringIO
from janome.tokenizer import Tokenizer

# --- 0. Gemini APIã‚­ãƒ¼ã®è¨­å®š ---
# Streamlitã®ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆç®¡ç†æ©Ÿèƒ½ã‹ã‚‰APIã‚­ãƒ¼ã‚’å–å¾—
try:
    ## ç’°å¢ƒå¤‰æ•°ã‹ã‚‰APIã‚­ãƒ¼èª­ã¿è¾¼ã¿
    gemini_api_key = os.environ["GEMINI_API_B2BAPP"]

    if gemini_api_key:
        genai.configure(api_key = gemini_api_key)
        ## st.success("OK:API kye")
    else:
        st.error("APIã‚­ãƒ¼ãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        st.stop("APIã‚­ãƒ¼ã‚¨ãƒ©ãƒ¼ã®ãŸã‚å‡¦ç†ã‚’åœæ­¢ã—ã¾ã™ã€‚")
except Exception as e:
    st.error(f"APIã‚­ãƒ¼è¨­å®šã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ:{e}")
    exit()

# --- 1. Janomeã®åˆæœŸåŒ– ---
try:
    janome_tokenizer = Tokenizer()
except Exception as e:
    st.error(f"Janomeã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚¨ãƒ©ãƒ¼: {e}")
    st.stop()


# --- 2. Big5æ€§æ ¼åˆ†æã®ãƒ­ã‚¸ãƒƒã‚¯ ---
PERSONALITY_WORDS = {
    'å¤–å‘æ€§': ['ã¿ã‚“ãª', 'æ¥½ã—ã„', 'ãƒ‘ãƒ¼ãƒ†ã‚£ãƒ¼', 'ä¼šã†', 'è©±ã™', 'æœ€é«˜', 'ï¼', 'ï¼ˆç¬‘ï¼‰'],
    'å”èª¿æ€§': ['å”åŠ›', 'ä¸€ç·’', 'æ‰‹ä¼ã†', 'ã‚ã‚ŠãŒã¨ã†', 'ãŠé¡˜ã„ã—ã¾ã™', 'æ„Ÿè¬', 'ç§ãŸã¡'],
    'èª å®Ÿæ€§': ['è¨ˆç”»', 'ç¢ºå®Ÿ', 'ã¹ã', 'é‡è¦', 'è²¬ä»»', 'ã—ã£ã‹ã‚Š', 'ç®¡ç†', 'å ±å‘Š'],
    'ç¥çµŒç—‡å‚¾å‘': ['å¿ƒé…', 'ä¸å®‰', 'å•é¡Œ', 'é›£ã—ã„', 'å¤§å¤‰', 'ãƒªã‚¹ã‚¯', 'ã™ã¿ã¾ã›ã‚“'],
    'é–‹æ”¾æ€§': ['æ–°ã—ã„', 'ã‚¢ã‚¤ãƒ‡ã‚¢', 'é¢ç™½ã„', 'è©¦ã™', 'æƒ³åƒ', 'ã‚ãã‚ã', 'ï¼Ÿ', 'ãªã‚‹ã»ã©']
}

## ãƒãƒ£ãƒƒãƒˆã®ãƒ†ã‚­ã‚¹ãƒˆã‚’Janomeã§å˜èªã«åˆ‡ã‚Šåˆ†ã‘
def analyze_personality(text):
    if not isinstance(text, str) or not text.strip():
        return {p: 0 for p in PERSONALITY_WORDS.keys()}
    # â†“ Janomeã®å‡¦ç†ã«ç½®ãæ›ãˆ
    words = [token.surface for token in janome_tokenizer.tokenize(text)]
    scores = {p: 0 for p in PERSONALITY_WORDS.keys()}
    for personality, keywords in PERSONALITY_WORDS.items():
        for word in keywords:
            scores[personality] += words.count(word)
    return scores

## æ–‡ç« ä¸­ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆã—æ€§æ ¼ã‚¿ã‚¤ãƒ—ã”ã¨ã®ã‚¹ã‚³ã‚¢ç®—å‡º
def get_dominant_personality(scores):
    if not scores or sum(scores.values()) == 0:
        return "åˆ†æä¸èƒ½"
    return max(scores, key=scores.get)

## æ€§æ ¼ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—ã™ã‚‹é–¢æ•°
def calculate_cosine_similarity(vec1, vec2):
    vec1 = np.array(vec1)
    vec2 = np.array(vec2)
    norm1 = np.linalg.norm(vec1)
    norm2 = np.linalg.norm(vec2)
    if norm1 == 0 or norm2 == 0:
        return 0.0
    dot_product = np.dot(vec1, vec2)
    similarity = dot_product / (norm1 * norm2)
    return similarity

## æ€§æ ¼ã‚¹ã‚³ã‚¢ã‚’å…ƒã«ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆã‚’ç”Ÿæˆ
def create_multi_user_radar_chart(result_df, my_name):
    labels = list(PERSONALITY_WORDS.keys())
    fig = go.Figure()

    # è‡ªåˆ†ä»¥å¤–ã®ãƒ¡ãƒ³ãƒãƒ¼ã«ä½¿ç”¨ã™ã‚‹è‰²ã®ãƒªã‚¹ãƒˆã‚’å®šç¾©
    other_colors = [
        '#1f77b4',  # é’
        '#2ca02c',  # ç·‘
        '#9467bd',  # ç´«
        '#ff7f0e',  # ã‚ªãƒ¬ãƒ³ã‚¸
        '#8c564b',  # èŒ¶è‰²
        '#e377c2',  # ãƒ”ãƒ³ã‚¯
        '#7f7f7f',  # ã‚°ãƒ¬ãƒ¼
        '#bcbd22',  # ã‚ªãƒªãƒ¼ãƒ–
        '#17becf'   # ã‚·ã‚¢ãƒ³
    ]
    color_index = 0

    score_details_df = result_df['ã‚¹ã‚³ã‚¢è©³ç´°'].apply(pd.Series)
    max_score = score_details_df.max().max() if not score_details_df.empty else 1

    for index, row in result_df.iterrows():
        user_name = row['ãƒ¦ãƒ¼ã‚¶ãƒ¼']
        scores = row['ã‚¹ã‚³ã‚¢è©³ç´°']
        values = list(scores.values())

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼åã«å¿œã˜ã¦è‰²ã‚’æ±ºå®š
        if user_name == my_name:
            line_color = 'red'
        else:
            line_color = other_colors[color_index % len(other_colors)]
            color_index += 1
        
        fig.add_trace(go.Scatterpolar(
            r=values,
            theta=labels,
            fill='toself',
            name=user_name,
            # line=dict(color=...) ã§ç·šã®è‰²ã‚’æŒ‡å®š
            line=dict(color=line_color),
            hovertemplate=f'<b>{user_name}</b><br>%{{theta}}: %{{r}}<extra></extra>'
        ))

    fig.update_layout(
        polar=dict(
            radialaxis=dict(
                visible=True,
                range=[0, max_score if max_score > 0 else 1]
            )),
        showlegend=True,
        title="å…¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ€§æ ¼ã‚¹ã‚³ã‚¢æ¯”è¼ƒ"
    )
    return fig

# --- 3. Gemini APIã‚’ç”¨ã„ãŸãƒšãƒ«ã‚½ãƒŠç”Ÿæˆé–¢æ•° ---
def generate_persona_with_retry(target_user_name, target_user_scores, target_user_text, my_scores, max_retries=3):
    """
    Gemini APIã‚’å‘¼ã³å‡ºã—ã¦ãƒšãƒ«ã‚½ãƒŠã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°ï¼ˆãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ã¨JSONãƒ‘ãƒ¼ã‚¹æ©Ÿèƒ½ä»˜ãï¼‰
    """
    ## Geminiã®ãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡å®š
    model = genai.GenerativeModel('gemini-1.5-flash-latest')
    
    ## è§£æã«å¤±æ•—ã—ãŸå ´åˆã«è¡¨ç¤ºã™ã‚‹å†…å®¹ã‚’ã‚ã‚‰ã‹ã˜ã‚å®šç¾©
    default_response = {
        "persona": "AIã«ã‚ˆã‚‹äººæŸ„ã®è§£æã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚",
        "common_point": "å…±é€šç‚¹ã®è§£æã«å¤±æ•—ã—ã¾ã—ãŸã€‚",
        "communication_point": "ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒã‚¤ãƒ³ãƒˆã®è§£æã«å¤±æ•—ã—ã¾ã—ãŸã€‚"
    }

    ## AIã¸ã®æŒ‡ç¤ºã‚’JSONå½¢å¼ã§å‡ºåŠ›
    prompt = f"""
ã‚ãªãŸã¯ã€å„ªç§€ãªçµ„ç¹”äººäº‹ã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®æƒ…å ±ã‚’åŸºã«ã€{target_user_name}ã•ã‚“ã®ãƒšãƒ«ã‚½ãƒŠã‚’åˆ†æã—ã€æŒ‡å®šã•ã‚ŒãŸJSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

# åˆ†æå¯¾è±¡è€…ã®æƒ…å ±
- æ°å: {target_user_name}
- æ€§æ ¼ã‚¹ã‚³ã‚¢: {target_user_scores}
- ãƒãƒ£ãƒƒãƒˆç™ºè¨€ã®æŠœç²‹: {target_user_text[:200]}

# æ¯”è¼ƒå¯¾è±¡è€…ï¼ˆè‡ªåˆ†ï¼‰ã®æƒ…å ±
- æ€§æ ¼ã‚¹ã‚³ã‚¢: {my_scores}

# å‡ºåŠ›å½¢å¼ã®ãƒ«ãƒ¼ãƒ«
- ä»¥ä¸‹ã®ã‚­ãƒ¼ã‚’æŒã¤JSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ã¿ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
- å„é …ç›®ã®å€¤ã¯ã€æŒ‡å®šã•ã‚ŒãŸæ–‡å­—æ•°ã§å³å¯†ã«ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
- JSONä»¥å¤–ã®æ–‡å­—åˆ—ï¼ˆä¾‹ãˆã° "```json" ã‚„ "```" ãªã©ï¼‰ã¯çµ¶å¯¾ã«å‡ºåŠ›ã«å«ã‚ãªã„ã§ãã ã•ã„ã€‚

{{
  "persona": "ï¼ˆ{target_user_name}ã•ã‚“ã®äººæŸ„ã‚’50æ–‡å­—ç¨‹åº¦ã§èª¬æ˜ï¼‰",
  "common_point": "ï¼ˆè‡ªåˆ†ã¨ã®æ€§æ ¼ã‚¹ã‚³ã‚¢ã®å…±é€šç‚¹ã‚’30æ–‡å­—ç¨‹åº¦ã§èª¬æ˜ï¼‰",
  "communication_point": "ï¼ˆå††æ»‘ãªé–¢ä¿‚ã‚’ç¯‰ããŸã‚ã®ãƒã‚¤ãƒ³ãƒˆã‚’50æ–‡å­—ç¨‹åº¦ã§èª¬æ˜ï¼‰"
}}
"""
    ## max_retriesã§æŒ‡å®šã—ãŸå›æ•°ã ã‘ã€æˆåŠŸã™ã‚‹ã¾ã§å‡¦ç†ã‚’è©¦ã¿
    for attempt in range(max_retries):
        try:
            response = model.generate_content(prompt)
            ## GeminiãŒå‡ºåŠ›ã™ã‚‹ã“ã¨ãŒã‚ã‚‹ä½™è¨ˆãªæ–‡å­—åˆ—ã‚’å‰Šé™¤
            cleaned_text = response.text.strip().replace("```json", "").replace("```", "").strip()
            ## AIã®å›ç­”ï¼ˆæ–‡å­—åˆ—ï¼‰ã‚’JSONå½¢å¼ï¼ˆè¾æ›¸ï¼‰ã«å¤‰æ›
            persona_data = json.loads(cleaned_text)
            
            ## å¿…è¦ãªé …ç›®ãŒã™ã¹ã¦å«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            if all(k in persona_data for k in ["persona", "common_point", "communication_point"]):
                return persona_data  ## æˆåŠŸã—ãŸã‚‰ã€çµæœã®è¾æ›¸ã‚’è¿”ã™
                
        except (json.JSONDecodeError, Exception) as e:
            ## å¤±æ•—ã—ãŸå ´åˆã¯ã€æ¬¡ã®è©¦è¡Œã¸
            print(f"Attempt {attempt + 1} failed: {e}")
            if attempt == max_retries - 1:
                ## ã™ã¹ã¦ã®è©¦è¡ŒãŒå¤±æ•—ã—ãŸå ´åˆ
                st.warning(f"AIã«ã‚ˆã‚‹ãƒšãƒ«ã‚½ãƒŠç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆè©¦è¡Œå›æ•°: {max_retries}å›ï¼‰ã€‚")
                return default_response
    
    return default_response

# --- 4. Streamlitã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ç”»é¢ ---
st.title('Teamsãƒãƒ£ãƒƒãƒˆ æ€§æ ¼åˆ†æã‚¢ãƒ—ãƒª ğŸ’¬')
st.write('ã‚¢ã‚µã‚¤ãƒ³äºˆå®šã®PJãƒ¡ãƒ³ãƒãƒ¼ã¨ã‚ãªãŸã®ãƒãƒ£ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿(CSV)ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã¨ã€æ€§æ ¼å‚¾å‘ã‚’åˆ†æã—ã€PJãƒ¡ãƒ³ãƒãƒ¼ã¨ã®ã€Œæ€§æ ¼ãƒãƒƒãƒåº¦ã€ã‚’è¨ºæ–­ã—ã¾ã™ã€‚')
st.write('---')

col1, col2 = st.columns(2)

## CSVã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ€ãƒ¼
with col1:
    st.subheader("PJãƒ¡ãƒ³ãƒãƒ¼ã®ãƒãƒ£ãƒƒãƒˆ")
    team_files = st.file_uploader(
        "CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¤‡æ•°é¸æŠã—ã¦ãã ã•ã„",
        type="csv",
        accept_multiple_files=True,
        key="team_uploader"
    )

with col2:
    st.subheader("è‡ªåˆ†ã®ãƒãƒ£ãƒƒãƒˆ")
    my_file = st.file_uploader(
        "CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’1ã¤é¸æŠã—ã¦ãã ã•ã„",
        type="csv",
        accept_multiple_files=False,
        key="my_uploader"
    )

st.write('---')

if team_files and my_file:
    try:
        ## ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å‡¦ç†
        team_dfs = []
        for file in team_files:
            file.seek(0)
            try:
                df_single = pd.read_csv(file, encoding='shift_jis')
            except UnicodeDecodeError:
                file.seek(0)
                df_single = pd.read_csv(file, encoding='utf-8')
            team_dfs.append(df_single)
        team_df = pd.concat(team_dfs, ignore_index=True)

        my_file.seek(0)
        try:
            my_df = pd.read_csv(my_file, encoding='shift_jis')
        except UnicodeDecodeError:
            my_file.seek(0)
            my_df = pd.read_csv(my_file, encoding='utf-8')

        my_name = ""
        if 'user' in my_df.columns and not my_df.empty:
            my_name = my_df['user'].iloc[0]
            st.info(f"ã‚ãªãŸã®ãƒ¦ãƒ¼ã‚¶ãƒ¼åã‚’ã€Œ{my_name}ã€ã¨ã—ã¦èªè­˜ã—ã¾ã—ãŸã€‚")
        else:
            st.error("ã‚ãªãŸã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã« 'user' åˆ—ãŒå­˜åœ¨ã—ãªã„ã‹ã€ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚")
            st.stop()
        
        df = pd.concat([team_df, my_df], ignore_index=True)
        
        if 'user' not in df.columns or 'message' not in df.columns:
            st.error("ã‚¨ãƒ©ãƒ¼: CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ã¯ 'user' ã¨ 'message' ã®åˆ—ãŒå¿…è¦ã§ã™ã€‚")
        else:
            st.success(f'{len(team_files) + 1}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«æˆåŠŸã—ã¾ã—ãŸï¼')
            
            with st.expander("èª­ã¿è¾¼ã‚“ã ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèªã™ã‚‹"):
                st.write("â–¼ ãƒãƒ¼ãƒ ãƒ¡ãƒ³ãƒãƒ¼ã®ãƒ‡ãƒ¼ã‚¿ï¼ˆå…ˆé ­5è¡Œï¼‰")
                st.dataframe(team_df.head())
                st.write("â–¼ ã‚ãªãŸã®ãƒ‡ãƒ¼ã‚¿ï¼ˆå…ˆé ­5è¡Œï¼‰")
                st.dataframe(my_df.head())

            ## ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦æ€§æ ¼ãƒãƒƒãƒåº¦åˆ†æã‚’å®Ÿè¡Œ
            if st.button('PJãƒ¡ãƒ³ãƒãƒ¼ã¨ã®æ€§æ ¼ãƒãƒƒãƒãƒ³ã‚°ã‚’ç¢ºèª'):
                st.write('---')
                st.header('åˆ†æçµæœ')
                df['message'] = df['message'].fillna('')
                user_texts = df.groupby('user')['message'].apply(' '.join).reset_index()
                
                results = []
                with st.spinner('æ€§æ ¼ã‚¹ã‚³ã‚¢ã‚’åˆ†æä¸­ã§ã™...'):
                    for index, row in user_texts.iterrows():
                        user = row['user']
                        text = row['message']
                        scores = analyze_personality(text)
                        dominant_personality = get_dominant_personality(scores)
                        results.append({
                            'ãƒ¦ãƒ¼ã‚¶ãƒ¼': user,
                            'æœ€ã‚‚å¼·ã„æ€§æ ¼å‚¾å‘': dominant_personality,
                            'ç™ºè¨€æ•°': df[df['user'] == user].shape[0],
                            'ã‚¹ã‚³ã‚¢è©³ç´°': scores,
                            'å…¨ç™ºè¨€': text
                        })
                
                result_df = pd.DataFrame(results)

                if my_name in result_df['ãƒ¦ãƒ¼ã‚¶ãƒ¼'].values:
                    my_scores_row = result_df[result_df['ãƒ¦ãƒ¼ã‚¶ãƒ¼'] == my_name].iloc[0]
                    my_scores = my_scores_row['ã‚¹ã‚³ã‚¢è©³ç´°']
                    my_scores_list = list(my_scores.values())

                    match_percentages = []
                    for index, row in result_df.iterrows():
                        if row['ãƒ¦ãƒ¼ã‚¶ãƒ¼'] == my_name:
                            match_percentages.append(100.0)
                            continue
                        other_scores_list = list(row['ã‚¹ã‚³ã‚¢è©³ç´°'].values())
                        similarity = calculate_cosine_similarity(my_scores_list, other_scores_list)
                        match_percentages.append(round(similarity * 100, 2))
                    
                    result_df['è‡ªåˆ†ã¨ã®æ€§æ ¼ãƒãƒƒãƒåº¦ (%)'] = match_percentages
                    
                    ## æ€§æ ¼ãƒãƒƒãƒåº¦åˆ†æçµæœã‚’è¡¨ç¤º
                    st.subheader(f'ğŸ‘¤ {my_name} ã¨PJãƒ¡ãƒ³ãƒãƒ¼ã®æ€§æ ¼ãƒãƒƒãƒåº¦')
                    st.dataframe(
                        result_df[['ãƒ¦ãƒ¼ã‚¶ãƒ¼', 'æœ€ã‚‚å¼·ã„æ€§æ ¼å‚¾å‘', 'ç™ºè¨€æ•°', 'è‡ªåˆ†ã¨ã®æ€§æ ¼ãƒãƒƒãƒåº¦ (%)']]
                        .sort_values('è‡ªåˆ†ã¨ã®æ€§æ ¼ãƒãƒƒãƒåº¦ (%)', ascending=False)
                    )
                else:
                    st.warning(f"'{my_name}'ã®ãƒ‡ãƒ¼ã‚¿ãŒåˆ†æçµæœã«å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã§ã—ãŸã€‚")
                    st.dataframe(result_df[['ãƒ¦ãƒ¼ã‚¶ãƒ¼', 'æœ€ã‚‚å¼·ã„æ€§æ ¼å‚¾å‘', 'ç™ºè¨€æ•°']])
                
                st.write('---')

                ## ãƒãƒ£ãƒ¼ãƒˆã‚’è¡¨ç¤º
                st.subheader('ğŸ“ˆ å…¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ€§æ ¼ã‚¹ã‚³ã‚¢æ¯”è¼ƒãƒãƒ£ãƒ¼ãƒˆ')
                if not result_df.empty:
                    fig = create_multi_user_radar_chart(result_df, my_name)
                    st.plotly_chart(fig, use_container_width=True)
                else:
                    st.write("åˆ†æå¯¾è±¡ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                
                with st.expander("å„ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚¹ã‚³ã‚¢è©³ç´°ã‚’è¦‹ã‚‹"):
                    score_details_df = result_df.set_index('ãƒ¦ãƒ¼ã‚¶ãƒ¼')['ã‚¹ã‚³ã‚¢è©³ç´°'].apply(pd.Series)
                    st.dataframe(score_details_df)

                ## ãƒšãƒ«ã‚½ãƒŠè¡¨ç¤ºæ©Ÿèƒ½
                st.write('---')
                st.subheader('ğŸ¤– AIã«ã‚ˆã‚‹ãƒšãƒ«ã‚½ãƒŠåˆ†æ')

                ## è‡ªåˆ†ä»¥å¤–ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’ãƒãƒƒãƒåº¦é †ã«ã‚½ãƒ¼ãƒˆ
                other_users_df = result_df[result_df['ãƒ¦ãƒ¼ã‚¶ãƒ¼'] != my_name].sort_values('è‡ªåˆ†ã¨ã®æ€§æ ¼ãƒãƒƒãƒåº¦ (%)', ascending=False)

                with st.spinner('AIãŒå„ãƒ¡ãƒ³ãƒãƒ¼ã®ãƒšãƒ«ã‚½ãƒŠã‚’ç”Ÿæˆä¸­ã§ã™...'):
                    # DataFrameã®è¡Œã‚’ãƒªã‚¹ãƒˆã«å¤‰æ›ã—ã¦ã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã§ã‚¢ã‚¯ã‚»ã‚¹ã—ã‚„ã™ãã—ã¾ã™
                    user_list = list(other_users_df.iterrows())
                    num_users = len(user_list)

                    # 2äººãšã¤ã®ãƒšã‚¢ã§å‡¦ç†ã™ã‚‹ãŸã‚ã®ãƒ«ãƒ¼ãƒ— (rangeã®3ç•ªç›®ã®å¼•æ•° `2` ãŒã‚¹ãƒ†ãƒƒãƒ—æ•°)
                    for i in range(0, num_users, 2):
                        # 2ã¤ã®ã‚«ãƒ©ãƒ ã‚’ä½œæˆ
                        col1, col2 = st.columns(2)

                        # --- 1äººç›®ã®ãƒšãƒ«ã‚½ãƒŠã‚’å·¦ã‚«ãƒ©ãƒ ã«è¡¨ç¤º ---
                        with col1:
                            # 1äººç›®ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ã‚’å–å¾—
                            index1, row1 = user_list[i]
                            target_name1 = row1['ãƒ¦ãƒ¼ã‚¶ãƒ¼']
                            target_scores1 = row1['ã‚¹ã‚³ã‚¢è©³ç´°']
                            target_text1 = row1['å…¨ç™ºè¨€']
                            
                            # expanderã‚’ä½¿ã£ã¦çµæœã‚’è¡¨ç¤º (expanded=Trueã§æœ€åˆã‹ã‚‰é–‹ã„ã¦ãŠã)
                            with st.expander(f"**{target_name1}ã•ã‚“** ã®ãƒšãƒ«ã‚½ãƒŠã‚’è¦‹ã‚‹", expanded=True):
                                persona_dict = generate_persona_with_retry(target_name1, target_scores1, target_text1, my_scores)
                                
                                st.markdown(f"**äººæŸ„**:")
                                st.info(persona_dict.get('persona', 'è§£æã§ãã¾ã›ã‚“ã§ã—ãŸã€‚'))
                                
                                st.markdown(f"**è‡ªåˆ†ã¨ã®å…±é€šç‚¹**:")
                                st.success(persona_dict.get('common_point', 'è§£æã§ãã¾ã›ã‚“ã§ã—ãŸã€‚'))

                                st.markdown(f"**ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒã‚¤ãƒ³ãƒˆ**:")
                                st.warning(persona_dict.get('communication_point', 'è§£æã§ãã¾ã›ã‚“ã§ã—ãŸã€‚'))

                        # --- 2äººç›®ã®ãƒšãƒ«ã‚½ãƒŠã‚’å³ã‚«ãƒ©ãƒ ã«è¡¨ç¤º (å­˜åœ¨ã™ã‚‹å ´åˆã®ã¿) ---
                        if (i + 1) < num_users:
                            with col2:
                                # 2äººç›®ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ã‚’å–å¾—
                                index2, row2 = user_list[i + 1]
                                target_name2 = row2['ãƒ¦ãƒ¼ã‚¶ãƒ¼']
                                target_scores2 = row2['ã‚¹ã‚³ã‚¢è©³ç´°']
                                target_text2 = row2['å…¨ç™ºè¨€']

                                with st.expander(f"**{target_name2}ã•ã‚“** ã®ãƒšãƒ«ã‚½ãƒŠã‚’è¦‹ã‚‹", expanded=True):
                                    persona_dict = generate_persona_with_retry(target_name2, target_scores2, target_text2, my_scores)
                                    
                                    st.markdown(f"**äººæŸ„**:")
                                    st.info(persona_dict.get('persona', 'è§£æã§ãã¾ã›ã‚“ã§ã—ãŸã€‚'))
                                    
                                    st.markdown(f"**è‡ªåˆ†ã¨ã®å…±é€šç‚¹**:")
                                    st.success(persona_dict.get('common_point', 'è§£æã§ãã¾ã›ã‚“ã§ã—ãŸã€‚'))

                                    st.markdown(f"**ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒã‚¤ãƒ³ãƒˆ**:")
                                    st.warning(persona_dict.get('communication_point', 'è§£æã§ãã¾ã›ã‚“ã§ã—ãŸã€‚'))

    except Exception as e:
        st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")