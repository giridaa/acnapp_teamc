# ====================================================================
#  Streamlit Cloud ç’°å¢ƒå•é¡Œã¸ã®æœ€çµ‚å¯¾ç­–ã‚³ãƒ¼ãƒ‰
# ====================================================================
import subprocess
import sys
import os
import streamlit as st

# Streamlit Cloudä¸Šã§å®Ÿè¡Œã•ã‚Œã¦ã„ã‚‹å ´åˆã®ã¿ã€ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®å¼·åˆ¶ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆã‚’å®Ÿè¡Œ
# (ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§ã¯ä¸è¦ãªãŸã‚)
if "STREAMLIT_SHARING_MODE" in os.environ:
    try:
        # pipã‚’ä½¿ã£ã¦google-generativeaiãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’æœ€æ–°ç‰ˆã«å¼·åˆ¶çš„ã«ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰
        subprocess.check_call([
            sys.executable, "-m", "pip", "install", "--upgrade", "google-generativeai"
        ])
        # æ›´æ–°ãŒå®Œäº†ã—ãŸã“ã¨ã‚’ãƒˆãƒ¼ã‚¹ãƒˆé€šçŸ¥ã§çŸ¥ã‚‰ã›ã‚‹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        st.toast("âœ… Geminiãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’æœ€æ–°ç‰ˆã«æ›´æ–°ã—ã¾ã—ãŸã€‚")
    except Exception as e:
        # ã‚‚ã—å¤±æ•—ã—ãŸå ´åˆã¯ã€ã‚¨ãƒ©ãƒ¼ã‚’è¡¨ç¤ºã—ã¦ã‚¢ãƒ—ãƒªã‚’åœæ­¢
        st.error(f"ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®å¼·åˆ¶ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        st.stop()
# ====================================================================
#  å¯¾ç­–ã‚³ãƒ¼ãƒ‰ã“ã“ã¾ã§
# ====================================================================

## ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import pandas as pd
import numpy as np
import plotly.graph_objects as go
import google.generativeai as genai
import json
from io import StringIO
from janome.tokenizer import Tokenizer


# --- 0. Gemini APIã‚­ãƒ¼ã®è¨­å®š ---
# Streamlitã®ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆç®¡ç†æ©Ÿèƒ½ã‹ã‚‰APIã‚­ãƒ¼ã‚’å–å¾—
try:
    ## ç’°å¢ƒå¤‰æ•°ã‹ã‚‰APIã‚­ãƒ¼èª­ã¿è¾¼ã¿
    gemini_api_key = os.environ["GEMINI_API_B2BAPP"]

    if gemini_api_key:
        genai.configure(api_key = gemini_api_key)
        ## st.success("OK:API kye")
    else:
        st.error("APIã‚­ãƒ¼ãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        st.stop("APIã‚­ãƒ¼ã‚¨ãƒ©ãƒ¼ã®ãŸã‚å‡¦ç†ã‚’åœæ­¢ã—ã¾ã™ã€‚")
except Exception as e:
    st.error(f"APIã‚­ãƒ¼è¨­å®šã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ:{e}")
    exit()

# --- 1. Janomeã®åˆæœŸåŒ– ---
try:
    janome_tokenizer = Tokenizer()
except Exception as e:
    st.error(f"Janomeã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚¨ãƒ©ãƒ¼: {e}")
    st.stop()


# --- 2. Big5æ€§æ ¼åˆ†æ + ACNç‹¬è‡ªæ€§æ ¼ã®ãƒ­ã‚¸ãƒƒã‚¯ ---
PERSONALITY_WORDS = {
    ## Big5
    'å¤–å‘æ€§': ['ã¿ã‚“ãª', 'æ¥½ã—ã„', 'ãƒ‘ãƒ¼ãƒ†ã‚£ãƒ¼', 'ä¼šã†', 'è©±ã™', 'æœ€é«˜', 'ï¼', 'ï¼ˆç¬‘ï¼‰'],
    'å”èª¿æ€§': ['å”åŠ›', 'ä¸€ç·’', 'æ‰‹ä¼ã†', 'ã‚ã‚ŠãŒã¨ã†', 'ãŠé¡˜ã„ã—ã¾ã™', 'æ„Ÿè¬', 'ç§ãŸã¡'],
    'èª å®Ÿæ€§': ['è¨ˆç”»', 'ç¢ºå®Ÿ', 'ã¹ã', 'é‡è¦', 'è²¬ä»»', 'ã—ã£ã‹ã‚Š', 'ç®¡ç†', 'å ±å‘Š'],
    'ç¥çµŒç—‡å‚¾å‘': ['å¿ƒé…', 'ä¸å®‰', 'å•é¡Œ', 'é›£ã—ã„', 'å¤§å¤‰', 'ãƒªã‚¹ã‚¯', 'ã™ã¿ã¾ã›ã‚“'],
    'é–‹æ”¾æ€§': ['æ–°ã—ã„', 'ã‚¢ã‚¤ãƒ‡ã‚¢', 'é¢ç™½ã„', 'è©¦ã™', 'æƒ³åƒ', 'ã‚ãã‚ã', 'ï¼Ÿ', 'ãªã‚‹ã»ã©'],
    ## ACNã‚ªãƒªã‚¸ãƒŠãƒ«
    'çŸ­æ°—': ['ã¾ã ', 'é…ã„', 'ãŠãã„', 'ã„ãã’', 'æ€¥ã’', 'ã„ãã', 'æ€¥ã', 'ã‚„ã°', 'æ—©ã'],
    'ãƒ‘ãƒ¯ãƒãƒ©': ['ãŠã„ãŠã„', 'ã¾ã˜ã‹ã‚ˆ', 'ã‚„ã‚Œ', 'ãµã–ã‘ã‚‹ãª']
}

## ãƒãƒ£ãƒƒãƒˆã®ãƒ†ã‚­ã‚¹ãƒˆã‚’Janomeã§å˜èªã«åˆ‡ã‚Šåˆ†ã‘
def analyze_personality(text):
    if not isinstance(text, str) or not text.strip():
        return {p: 0 for p in PERSONALITY_WORDS.keys()}
    # â†“ Janomeã®å‡¦ç†ã«ç½®ãæ›ãˆ
    words = [token.surface for token in janome_tokenizer.tokenize(text)]
    scores = {p: 0 for p in PERSONALITY_WORDS.keys()}
    for personality, keywords in PERSONALITY_WORDS.items():
        for word in keywords:
            scores[personality] += words.count(word)
    return scores

## æ–‡ç« ä¸­ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆã—æ€§æ ¼ã‚¿ã‚¤ãƒ—ã”ã¨ã®ã‚¹ã‚³ã‚¢ç®—å‡º
def get_dominant_personality(scores):
    if not scores or sum(scores.values()) == 0:
        return "åˆ†æä¸èƒ½"
    return max(scores, key=scores.get)

## æ€§æ ¼ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—ã™ã‚‹é–¢æ•°
def calculate_cosine_similarity(vec1, vec2):
    vec1 = np.array(vec1)
    vec2 = np.array(vec2)
    norm1 = np.linalg.norm(vec1)
    norm2 = np.linalg.norm(vec2)
    if norm1 == 0 or norm2 == 0:
        return 0.0
    dot_product = np.dot(vec1, vec2)
    similarity = dot_product / (norm1 * norm2)
    return similarity

## æ€§æ ¼ã‚¹ã‚³ã‚¢ã‚’å…ƒã«ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆã‚’ç”Ÿæˆ
def create_multi_user_radar_chart(result_df, my_name):
    labels = list(PERSONALITY_WORDS.keys())
    fig = go.Figure()

    # è‡ªåˆ†ä»¥å¤–ã®ãƒ¡ãƒ³ãƒãƒ¼ã«ä½¿ç”¨ã™ã‚‹è‰²ã®ãƒªã‚¹ãƒˆã‚’å®šç¾©
    other_colors = [
        '#1f77b4',  # é’
        '#2ca02c',  # ç·‘
        '#9467bd',  # ç´«
        '#ff7f0e',  # ã‚ªãƒ¬ãƒ³ã‚¸
        '#8c564b',  # èŒ¶è‰²
        '#e377c2',  # ãƒ”ãƒ³ã‚¯
        '#7f7f7f',  # ã‚°ãƒ¬ãƒ¼
        '#bcbd22',  # ã‚ªãƒªãƒ¼ãƒ–
        '#17becf'   # ã‚·ã‚¢ãƒ³
    ]
    color_index = 0

    score_details_df = result_df['ã‚¹ã‚³ã‚¢è©³ç´°'].apply(pd.Series)
    max_score = score_details_df.max().max() if not score_details_df.empty else 1

    for index, row in result_df.iterrows():
        user_name = row['ãƒ¦ãƒ¼ã‚¶ãƒ¼']
        scores = row['ã‚¹ã‚³ã‚¢è©³ç´°']
        values = list(scores.values())

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼åã«å¿œã˜ã¦è‰²ã‚’æ±ºå®š
        if user_name == my_name:
            line_color = 'red'
        else:
            line_color = other_colors[color_index % len(other_colors)]
            color_index += 1
        
        fig.add_trace(go.Scatterpolar(
            r=values,
            theta=labels,
            fill='toself',
            name=user_name,
            # line=dict(color=...) ã§ç·šã®è‰²ã‚’æŒ‡å®š
            line=dict(color=line_color),
            hovertemplate=f'<b>{user_name}</b><br>%{{theta}}: %{{r}}<extra></extra>'
        ))

    fig.update_layout(
        polar=dict(
            radialaxis=dict(
                visible=True,
                range=[0, max_score if max_score > 0 else 1]
            )),
        showlegend=True,
        title="å…¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ€§æ ¼ã‚¹ã‚³ã‚¢æ¯”è¼ƒ"
    )
    return fig

# --- 3. Gemini APIã‚’ç”¨ã„ãŸãƒšãƒ«ã‚½ãƒŠç”Ÿæˆé–¢æ•° ---
def generate_persona_with_retry(target_user_name, target_user_scores, target_user_text, my_scores, max_retries=3):
    """
    Gemini APIã‚’å‘¼ã³å‡ºã—ã¦ãƒšãƒ«ã‚½ãƒŠã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°ï¼ˆãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ã¨JSONãƒ‘ãƒ¼ã‚¹æ©Ÿèƒ½ä»˜ãï¼‰
    """
    ## Geminiã®ãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡å®š
    model = genai.GenerativeModel('gemini-2.5-flash-lite')
    
    ## è§£æã«å¤±æ•—ã—ãŸå ´åˆã«è¡¨ç¤ºã™ã‚‹å†…å®¹ã‚’ã‚ã‚‰ã‹ã˜ã‚å®šç¾©
    default_response = {
        "persona": "AIã«ã‚ˆã‚‹äººæŸ„ã®è§£æã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚",
        "common_point": "å…±é€šç‚¹ã®è§£æã«å¤±æ•—ã—ã¾ã—ãŸã€‚",
        "communication_point": "ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒã‚¤ãƒ³ãƒˆã®è§£æã«å¤±æ•—ã—ã¾ã—ãŸã€‚"
    }

    prompt = f"""
    ã‚ãªãŸã¯ã€å„ªç§€ãªçµ„ç¹”äººäº‹ã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆã§ã™ã€‚
    ä»¥ä¸‹ã®æƒ…å ±ã‚’åŸºã«ã€{target_user_name}ã•ã‚“ã®ãƒšãƒ«ã‚½ãƒŠã‚’åˆ†æã—ã¦ãã ã•ã„ã€‚

    # åˆ†æå¯¾è±¡è€…ã®æƒ…å ±
    - æ°å: {target_user_name}
    - æ€§æ ¼ã‚¹ã‚³ã‚¢: {target_user_scores}
    - ãƒãƒ£ãƒƒãƒˆç™ºè¨€ã®æŠœç²‹: {target_user_text[:200]}

    # æ¯”è¼ƒå¯¾è±¡è€…ï¼ˆè‡ªåˆ†ï¼‰ã®æƒ…å ±
    - æ€§æ ¼ã‚¹ã‚³ã‚¢: {my_scores}

    #ã€æœ€é‡è¦ãƒ«ãƒ¼ãƒ«ã€‘
    - **å¿…ãšã€ä»¥ä¸‹ã®ã‚­ãƒ¼ã‚’æŒã¤JSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ"ã ã‘"ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚**
    - **è§£èª¬ã‚„å‰ç½®ãã€```jsonã®ã‚ˆã†ãªè¿½åŠ ã®æ–‡å­—åˆ—ã¯çµ¶å¯¾ã«å«ã‚ãªã„ã§ãã ã•ã„ã€‚**

    {{
    "persona": "ï¼ˆ{target_user_name}ã•ã‚“ã®äººæŸ„ã‚’50æ–‡å­—ç¨‹åº¦ã§èª¬æ˜ï¼‰",
    "common_point": "ï¼ˆè‡ªåˆ†ã¨ã®æ€§æ ¼ã‚¹ã‚³ã‚¢ã®å…±é€šç‚¹ã‚’30æ–‡å­—ç¨‹åº¦ã§èª¬æ˜ï¼‰",
    "communication_point": "ï¼ˆå††æ»‘ãªé–¢ä¿‚ã‚’ç¯‰ããŸã‚ã®ãƒã‚¤ãƒ³ãƒˆã‚’50æ–‡å­—ç¨‹åº¦ã§èª¬æ˜ï¼‰"
    }}
    """
    ## max_retriesã§æŒ‡å®šã—ãŸå›æ•°ã ã‘ã€æˆåŠŸã™ã‚‹ã¾ã§å‡¦ç†ã‚’è©¦ã¿
    for attempt in range(max_retries):
        try:
            response = model.generate_content(prompt)
            ## GeminiãŒå‡ºåŠ›ã™ã‚‹ã“ã¨ãŒã‚ã‚‹ä½™è¨ˆãªæ–‡å­—åˆ—ã‚’å‰Šé™¤
            cleaned_text = response.text.strip().replace("```json", "").replace("```", "").strip()
            ## AIã®å›ç­”ï¼ˆæ–‡å­—åˆ—ï¼‰ã‚’JSONå½¢å¼ï¼ˆè¾æ›¸ï¼‰ã«å¤‰æ›
            persona_data = json.loads(cleaned_text)
            
            ## å¿…è¦ãªé …ç›®ãŒã™ã¹ã¦å«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            if all(k in persona_data for k in ["persona", "common_point", "communication_point"]):
                return persona_data  ## æˆåŠŸã—ãŸã‚‰ã€çµæœã®è¾æ›¸ã‚’è¿”ã™
                
        except (json.JSONDecodeError, Exception) as e:
            st.error(f"AIã‹ã‚‰ã®å¿œç­”å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸï¼ˆè©¦è¡Œ {attempt + 1}å›ç›®ï¼‰")
            st.error(f"ã‚¨ãƒ©ãƒ¼å†…å®¹: {e}")
            # responseå¤‰æ•°ãŒå­˜åœ¨ã—ã€textå±æ€§ã‚’æŒã¤ã‹ç¢ºèª
            if 'response' in locals() and hasattr(response, 'text'):
                st.text_area("AIã‹ã‚‰ã®ç”Ÿã®å¿œç­”:", response.text, height=150)
            
            if attempt == max_retries - 1:
                st.warning(f"AIã«ã‚ˆã‚‹ãƒšãƒ«ã‚½ãƒŠç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆè©¦è¡Œå›æ•°: {max_retries}å›ï¼‰ã€‚")
                return default_response
    
    return default_response

##ã“ã“ã‹ã‚‰ä¿®æ­£
# --- 3-2. Gemini APIã‚’ç”¨ã„ãŸãƒãƒ¼ãƒ é›°å›²æ°—åˆ†æé–¢æ•° ---
def generate_team_atmosphere(text, max_retries=3):
    """
    MTGã®ä¼šè©±ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãƒãƒ¼ãƒ ã®é›°å›²æ°—ã‚’åˆ†æã™ã‚‹é–¢æ•°
    """
    model = genai.GenerativeModel('gemini-2.5-flash-lite')
    
    # è§£æå¤±æ•—æ™‚ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå¿œç­”
    default_response = {
        "atmosphere": "åˆ†æå¤±æ•—",
        "description": "AIã«ã‚ˆã‚‹åˆ†æã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ†ã‚­ã‚¹ãƒˆãŒçŸ­ã™ãã‚‹ã‹ã€å†…å®¹ãŒä¸é©åˆ‡ã§ãªã„ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚",
        "weather": "éœ§"
    }

    # å¤©æ°—ã®é¸æŠè‚¢ã‚’å®šç¾©
    weather_options = ['å¿«æ™´', 'æ™´ã‚Œ', 'è–„æ›‡ã‚Š', 'æ›‡ã‚Š', 'é›¨', 'é›ª', 'é›·', 'éœ§', 'æš´é¢¨']

    prompt = f"""
    ã‚ãªãŸã¯ã€çµŒé¨“è±Šå¯Œãªçµ„ç¹”é–‹ç™ºã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆã§ã™ã€‚
    ä»¥ä¸‹ã®ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒŸãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®ä¼šè©±ãƒ†ã‚­ã‚¹ãƒˆå…¨ä½“ã‹ã‚‰ã€ãƒãƒ¼ãƒ å…¨ä½“ã®é›°å›²æ°—ã‚’åˆ†æã—ã¦ãã ã•ã„ã€‚

    # åˆ†æå¯¾è±¡ã®ä¼šè©±ãƒ†ã‚­ã‚¹ãƒˆï¼ˆæŠœç²‹ï¼‰
    {text[:4000]}

    #ã€æœ€é‡è¦ãƒ«ãƒ¼ãƒ«ã€‘
    - **å¿…ãšã€ä»¥ä¸‹ã®ã‚­ãƒ¼ã‚’æŒã¤JSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ"ã ã‘"ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚**
    - **è§£èª¬ã‚„å‰ç½®ãã€```jsonã®ã‚ˆã†ãªè¿½åŠ ã®æ–‡å­—åˆ—ã¯çµ¶å¯¾ã«å«ã‚ãªã„ã§ãã ã•ã„ã€‚**
    - **"weather"ã®å€¤ã¯ã€å¿…ãšä»¥ä¸‹ã®ãƒªã‚¹ãƒˆã‹ã‚‰æœ€ã‚‚é©åˆ‡ã ã¨æ€ã†ã‚‚ã®ã‚’1ã¤ã ã‘é¸ã‚“ã§ãã ã•ã„ã€‚**
      {weather_options}

    {{
      "atmosphere": "ï¼ˆãƒãƒ¼ãƒ ã®é›°å›²æ°—ã‚’15æ–‡å­—ç¨‹åº¦ã§è¡¨ç¾ï¼‰",
      "description": "ï¼ˆãªãœãã®é›°å›²æ°—ã ã¨åˆ¤æ–­ã—ãŸã‹ã€ç†ç”±ã‚’30æ–‡å­—ç¨‹åº¦ã§èª¬æ˜ï¼‰",
      "weather": "ï¼ˆä¸Šè¨˜ã®ãƒªã‚¹ãƒˆã‹ã‚‰é¸æŠã—ãŸå¤©æ°—ï¼‰"
    }}
    """
    for attempt in range(max_retries):
        try:
            response = model.generate_content(prompt)
            cleaned_text = response.text.strip().replace("```json", "").replace("```", "").strip()
            atmosphere_data = json.loads(cleaned_text)
            
            if all(k in atmosphere_data for k in ["atmosphere", "description", "weather"]):
                return atmosphere_data # æˆåŠŸ
        except (json.JSONDecodeError, Exception) as e:
            st.error(f"ãƒãƒ¼ãƒ é›°å›²æ°—ã®AIåˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸï¼ˆè©¦è¡Œ {attempt + 1}å›ç›®ï¼‰: {e}")
            if attempt == max_retries - 1:
                st.warning(f"AIã«ã‚ˆã‚‹ãƒãƒ¼ãƒ é›°å›²æ°—ã®åˆ†æã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                return default_response
    
    return default_response

def get_weather_icon(weather_str):
    """ å¤©æ°—ã®æ–‡å­—åˆ—ã«å¯¾å¿œã™ã‚‹çµµæ–‡å­—ã‚’è¿”ã™ """
    weather_map = {
        'å¿«æ™´': 'â˜€ï¸', 'æ™´ã‚Œ': 'æ™´ï¸', 'è–„æ›‡ã‚Š': 'ğŸŒ¥ï¸', 'æ›‡ã‚Š': 'â˜ï¸',
        'é›¨': 'ğŸŒ§ï¸', 'é›ª': 'â„ï¸', 'é›·': 'âš¡ï¸', 'éœ§': 'ğŸŒ«ï¸', 'æš´é¢¨': 'ğŸŒªï¸'
    }
    return weather_map.get(weather_str, 'â“') # è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€Œ?ã€ã‚’è¿”ã™
##ä¿®æ­£ã“ã“ã¾ã§

# --- 4. Streamlitã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ç”»é¢ ---
st.title('ã‚¢ã‚µã‚¤ãƒ³æ¤œè¨PJ æ€§æ ¼åˆ†æã‚¢ãƒ—ãƒª ğŸ’¬')
st.write('ã‚¢ã‚µã‚¤ãƒ³äºˆå®šã®PJãƒ¡ãƒ³ãƒãƒ¼ã®ãƒãƒ£ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã€MTGä¼šè©±ãƒ‡ãƒ¼ã‚¿ã¨ã‚ãªãŸã®ãƒãƒ£ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿(CSV)ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã¨ã€æ€§æ ¼å‚¾å‘ã‚’åˆ†æã—ã€PJãƒ¡ãƒ³ãƒãƒ¼ã¨ã®ã€Œæ€§æ ¼ãƒãƒƒãƒåº¦ã€ã‚’è¨ºæ–­ã—ã¾ã™ã€‚')
st.write('---')

col1, col2, col3 = st.columns(3)

## CSVã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ€ãƒ¼
with col1:
    st.subheader("ğŸ‘¥PJãƒ¡ãƒ³ãƒãƒ¼ã®Teamsãƒãƒ£ãƒƒãƒˆ")
    # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’å—ã‘å–ã‚‹å¤‰æ•°ã‚’å¤‰æ›´
    chat_files = st.file_uploader(
        "PJã®ãƒãƒ£ãƒƒãƒˆCSVã‚’é¸æŠã—ã¦ãã ã•ã„",
        type="csv",
        accept_multiple_files=True,
        key="chat_uploader"
    )

with col2:
    st.subheader("ğŸ‘¥PJãƒ¡ãƒ³ãƒãƒ¼ã®MTGä¼šè©±")
    # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’å—ã‘å–ã‚‹å¤‰æ•°ã‚’å¤‰æ›´
    transcript_files = st.file_uploader(
        "éŸ³å£°ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆå¤‰æ›ã—ãŸCSVã‚’é¸æŠã—ã¦ãã ã•ã„",
        type="csv",
        accept_multiple_files=True,
        key="transcript_uploader"
    )

with col3:
    st.subheader("ğŸ‘¤è‡ªåˆ†ã®Teamsãƒãƒ£ãƒƒãƒˆ")
    my_file = st.file_uploader(
        "è‡ªåˆ†ã®ãƒãƒ£ãƒƒãƒˆã®CSVã‚’é¸æŠã—ã¦ãã ã•ã„",
        type="csv",
        accept_multiple_files=False,
        key="mychat_uploader"
    )

st.write('---')

# ãƒãƒ¼ãƒ ãƒ¡ãƒ³ãƒãƒ¼ã®ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆãƒãƒ£ãƒƒãƒˆ or MTGï¼‰ãŒã©ã¡ã‚‰ã‹ä¸€æ–¹ã§ã‚‚ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸã‚‰å‡¦ç†ã«é€²ã‚€ã‚ˆã†ã«æ¡ä»¶ã‚’å¤‰æ›´
if (chat_files or transcript_files) and my_file:
    try:
##ã“ã“ã‹ã‚‰ä¿®æ­£
        # --- ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿å‡¦ç† ---
        # æ€§æ ¼åˆ†æç”¨ã®ãƒãƒ£ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€
        team_chat_dfs = []
        if chat_files:
            for file in chat_files:
                file.seek(0)
                try:
                    df_single = pd.read_csv(file, encoding='shift_jis')
                except UnicodeDecodeError:
                    file.seek(0)
                    df_single = pd.read_csv(file, encoding='utf-8')
                team_chat_dfs.append(df_single)
        
        team_chat_df = pd.DataFrame()
        if team_chat_dfs:
            team_chat_df = pd.concat(team_chat_dfs, ignore_index=True)

        # ãƒãƒ¼ãƒ é›°å›²æ°—åˆ†æç”¨ã®MTGä¼šè©±ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€
        team_transcript_dfs = []
        transcript_text = ""
        if transcript_files:
            for file in transcript_files:
                file.seek(0)
                try:
                    df_single = pd.read_csv(file, encoding='shift_jis')
                except UnicodeDecodeError:
                    file.seek(0)
                    df_single = pd.read_csv(file, encoding='utf-8')
                team_transcript_dfs.append(df_single)
        
        team_transcript_df = pd.DataFrame()
        if team_transcript_dfs:
            team_transcript_df = pd.concat(team_transcript_dfs, ignore_index=True)
            if 'message' in team_transcript_df.columns:
                # å…¨ã¦ã®ç™ºè¨€ã‚’ä¸€ã¤ã®ãƒ†ã‚­ã‚¹ãƒˆã«çµåˆ
                transcript_text = ' '.join(team_transcript_df['message'].fillna('').astype(str))

        # è‡ªåˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€
        my_file.seek(0)
        try:
            my_df = pd.read_csv(my_file, encoding='shift_jis')
        except UnicodeDecodeError:
            my_file.seek(0)
            my_df = pd.read_csv(my_file, encoding='utf-8')
##ä¿®æ­£ã“ã“ã¾ã§

        my_name = ""
        if 'user' in my_df.columns and not my_df.empty:
            my_name = my_df['user'].iloc[0]
            st.info(f"ã‚ãªãŸã®ãƒ¦ãƒ¼ã‚¶ãƒ¼åã‚’ã€Œ{my_name}ã€ã¨ã—ã¦èªè­˜ã—ã¾ã—ãŸã€‚")
        else:
            st.error("ã‚ãªãŸã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã« 'user' åˆ—ãŒå­˜åœ¨ã—ãªã„ã‹ã€ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚")
            st.stop()
        
        # æ€§æ ¼åˆ†æã«ã¯ãƒãƒ£ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã¨è‡ªåˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
        df = pd.concat([team_chat_df, my_df], ignore_index=True)
        
        if 'user' not in df.columns or 'message' not in df.columns:
            # ãƒãƒ£ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã¯è­¦å‘Šã«ã¨ã©ã‚ã‚‹
            if not chat_files:
                 st.warning("æ€§æ ¼åˆ†æã®å¯¾è±¡ã¨ãªã‚‹PJãƒ¡ãƒ³ãƒãƒ¼ã®ãƒãƒ£ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
            else:
                 st.error("ã‚¨ãƒ©ãƒ¼: ãƒãƒ£ãƒƒãƒˆCSVãƒ•ã‚¡ã‚¤ãƒ«ã«ã¯ 'user' ã¨ 'message' ã®åˆ—ãŒå¿…è¦ã§ã™ã€‚")
        else:
            st.success(f'{len(chat_files) + len(transcript_files) + 1}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«æˆåŠŸã—ã¾ã—ãŸï¼')
            
            with st.expander("èª­ã¿è¾¼ã‚“ã ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèªã™ã‚‹"):
                if not team_chat_df.empty:
                    st.write("â–¼ ãƒãƒ¼ãƒ ãƒ¡ãƒ³ãƒãƒ¼ã®ãƒãƒ£ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆå…ˆé ­5è¡Œï¼‰")
                    st.dataframe(team_chat_df.head())
                if not team_transcript_df.empty:
                    st.write("â–¼ ãƒãƒ¼ãƒ ãƒ¡ãƒ³ãƒãƒ¼ã®MTGä¼šè©±ãƒ‡ãƒ¼ã‚¿ï¼ˆå…ˆé ­5è¡Œï¼‰")
                    st.dataframe(team_transcript_df.head())
                st.write("â–¼ ã‚ãªãŸã®ãƒ‡ãƒ¼ã‚¿ï¼ˆå…ˆé ­5è¡Œï¼‰")
                st.dataframe(my_df.head())

        ## ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦æ€§æ ¼ãƒãƒƒãƒåº¦åˆ†æã‚’å®Ÿè¡Œ
        if st.button('åˆ†æã‚’å®Ÿè¡Œã™ã‚‹'):
            st.write('---')
            st.header('åˆ†æçµæœ')

##ã“ã“ã‹ã‚‰ä¿®æ­£
            # --- ãƒãƒ¼ãƒ ã®é›°å›²æ°—åˆ†æï¼ˆMTGãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã®ã¿å®Ÿè¡Œï¼‰---
            if transcript_text:
                st.subheader('ğŸ—£ï¸ MTGã®ä¼šè©±ã‹ã‚‰åˆ†æã—ãŸãƒãƒ¼ãƒ ã®é›°å›²æ°—')
                with st.spinner('AIãŒãƒãƒ¼ãƒ ã®é›°å›²æ°—ã‚’åˆ†æä¸­ã§ã™...'):
                    atmosphere_result = generate_team_atmosphere(transcript_text)
                    
                    # å¤©æ°—ã‚’ã‚¢ã‚¤ã‚³ãƒ³ã§è¡¨ç¤º
                    weather_str = atmosphere_result.get('weather', 'éœ§')
                    weather_icon = get_weather_icon(weather_str)
                    
                    # çµæœã‚’3ã‚«ãƒ©ãƒ ã§è¡¨ç¤º
                    col_atm1, col_atm2, col_atm3 = st.columns(3)
                    with col_atm1:
                        st.metric(label="ç¾åœ¨ã®ãƒãƒ¼ãƒ ã®å¤©æ°—", value=weather_str, delta=weather_icon)
                    with col_atm2:
                        st.markdown("**ãƒãƒ¼ãƒ ã®é›°å›²æ°—**")
                        st.info(f"{atmosphere_result.get('atmosphere', 'N/A')}")
                    with col_atm3:
                        st.markdown("**é›°å›²æ°—ã®ç†ç”±**")
                        st.success(f"{atmosphere_result.get('description', 'N/A')}")
                st.write('---')
##ä¿®æ­£ã“ã“ã¾ã§
            
            # --- æ€§æ ¼ãƒãƒƒãƒåº¦åˆ†æï¼ˆãƒãƒ£ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã®ã¿å®Ÿè¡Œï¼‰---
            if not df.empty and 'user' in df.columns and df['user'].nunique() > 1:
                df['message'] = df['message'].fillna('')
                user_texts = df.groupby('user')['message'].apply(' '.join).reset_index()
                
                results = []
                with st.spinner('æ€§æ ¼ã‚¹ã‚³ã‚¢ã‚’åˆ†æä¸­ã§ã™...'):
                    for index, row in user_texts.iterrows():
                        user = row['user']
                        text = row['message']
                        scores = analyze_personality(text)
                        dominant_personality = get_dominant_personality(scores)
                        results.append({
                            'ãƒ¦ãƒ¼ã‚¶ãƒ¼': user,
                            'æœ€ã‚‚å¼·ã„æ€§æ ¼å‚¾å‘': dominant_personality,
                            'ç™ºè¨€æ•°': df[df['user'] == user].shape[0],
                            'ã‚¹ã‚³ã‚¢è©³ç´°': scores,
                            'å…¨ç™ºè¨€': text
                        })
                
                result_df = pd.DataFrame(results)

                if my_name in result_df['ãƒ¦ãƒ¼ã‚¶ãƒ¼'].values:
                    my_scores_row = result_df[result_df['ãƒ¦ãƒ¼ã‚¶ãƒ¼'] == my_name].iloc[0]
                    my_scores = my_scores_row['ã‚¹ã‚³ã‚¢è©³ç´°']
                    my_scores_list = list(my_scores.values())

                    match_percentages = []
                    for index, row in result_df.iterrows():
                        if row['ãƒ¦ãƒ¼ã‚¶ãƒ¼'] == my_name:
                            match_percentages.append(100.0)
                            continue
                        other_scores_list = list(row['ã‚¹ã‚³ã‚¢è©³ç´°'].values())
                        similarity = calculate_cosine_similarity(my_scores_list, other_scores_list)
                        match_percentages.append(round(similarity * 100, 2))
                    
                    result_df['è‡ªåˆ†ã¨ã®æ€§æ ¼ãƒãƒƒãƒåº¦ (%)'] = match_percentages
                    
                    ## æ€§æ ¼ãƒãƒƒãƒåº¦åˆ†æçµæœã‚’è¡¨ç¤º
                    st.subheader(f'ğŸ‘¤ {my_name} ã¨PJãƒ¡ãƒ³ãƒãƒ¼ã®æ€§æ ¼ãƒãƒƒãƒåº¦')
                    st.dataframe(
                        result_df[['ãƒ¦ãƒ¼ã‚¶ãƒ¼', 'æœ€ã‚‚å¼·ã„æ€§æ ¼å‚¾å‘', 'ç™ºè¨€æ•°', 'è‡ªåˆ†ã¨ã®æ€§æ ¼ãƒãƒƒãƒåº¦ (%)']]
                        .sort_values('è‡ªåˆ†ã¨ã®æ€§æ ¼ãƒãƒƒãƒåº¦ (%)', ascending=False)
                    )
                else:
                    st.warning(f"'{my_name}'ã®ãƒ‡ãƒ¼ã‚¿ãŒåˆ†æçµæœã«å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã§ã—ãŸã€‚")
                    st.dataframe(result_df[['ãƒ¦ãƒ¼ã‚¶ãƒ¼', 'æœ€ã‚‚å¼·ã„æ€§æ ¼å‚¾å‘', 'ç™ºè¨€æ•°']])
                
                st.write('---')

                ## ãƒãƒ£ãƒ¼ãƒˆã‚’è¡¨ç¤º
                st.subheader('ğŸ“ˆ å…¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ€§æ ¼ã‚¹ã‚³ã‚¢æ¯”è¼ƒãƒãƒ£ãƒ¼ãƒˆ')
                if not result_df.empty:
                    fig = create_multi_user_radar_chart(result_df, my_name)
                    st.plotly_chart(fig, use_container_width=True)
                else:
                    st.write("åˆ†æå¯¾è±¡ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                
                with st.expander("å„ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚¹ã‚³ã‚¢è©³ç´°ã‚’è¦‹ã‚‹"):
                    score_details_df = result_df.set_index('ãƒ¦ãƒ¼ã‚¶ãƒ¼')['ã‚¹ã‚³ã‚¢è©³ç´°'].apply(pd.Series)
                    st.dataframe(score_details_df)

                ## ãƒšãƒ«ã‚½ãƒŠè¡¨ç¤ºæ©Ÿèƒ½
                st.write('---')
                st.subheader('ğŸ¤– AIã«ã‚ˆã‚‹ãƒšãƒ«ã‚½ãƒŠåˆ†æ')

                ## è‡ªåˆ†ä»¥å¤–ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’ãƒãƒƒãƒåº¦é †ã«ã‚½ãƒ¼ãƒˆ
                other_users_df = result_df[result_df['ãƒ¦ãƒ¼ã‚¶ãƒ¼'] != my_name].sort_values('è‡ªåˆ†ã¨ã®æ€§æ ¼ãƒãƒƒãƒåº¦ (%)', ascending=False)

                with st.spinner('AIãŒå„ãƒ¡ãƒ³ãƒãƒ¼ã®ãƒšãƒ«ã‚½ãƒŠã‚’ç”Ÿæˆä¸­ã§ã™...'):
                    # DataFrameã®è¡Œã‚’ãƒªã‚¹ãƒˆã«å¤‰æ›ã—ã¦ã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã§ã‚¢ã‚¯ã‚»ã‚¹ã—ã‚„ã™ãã—ã¾ã™
                    user_list = list(other_users_df.iterrows())
                    num_users = len(user_list)

                    # 2äººãšã¤ã®ãƒšã‚¢ã§å‡¦ç†ã™ã‚‹ãŸã‚ã®ãƒ«ãƒ¼ãƒ— (rangeã®3ç•ªç›®ã®å¼•æ•° `2` ãŒã‚¹ãƒ†ãƒƒãƒ—æ•°)
                    for i in range(0, num_users, 2):
                        # 2ã¤ã®ã‚«ãƒ©ãƒ ã‚’ä½œæˆ
                        col1, col2 = st.columns(2)

                        # --- 1äººç›®ã®ãƒšãƒ«ã‚½ãƒŠã‚’å·¦ã‚«ãƒ©ãƒ ã«è¡¨ç¤º ---
                        with col1:
                            # 1äººç›®ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ã‚’å–å¾—
                            index1, row1 = user_list[i]
                            target_name1 = row1['ãƒ¦ãƒ¼ã‚¶ãƒ¼']
                            target_scores1 = row1['ã‚¹ã‚³ã‚¢è©³ç´°']
                            target_text1 = row1['å…¨ç™ºè¨€']
                            
                            # expanderã‚’ä½¿ã£ã¦çµæœã‚’è¡¨ç¤º (expanded=Trueã§æœ€åˆã‹ã‚‰é–‹ã„ã¦ãŠã)
                            with st.expander(f"**{target_name1}ã•ã‚“** ã®ãƒšãƒ«ã‚½ãƒŠã‚’è¦‹ã‚‹", expanded=True):
                                persona_dict = generate_persona_with_retry(target_name1, target_scores1, target_text1, my_scores)
                                
                                st.markdown(f"**äººæŸ„**:")
                                st.info(persona_dict.get('persona', 'è§£æã§ãã¾ã›ã‚“ã§ã—ãŸã€‚'))
                                
                                st.markdown(f"**è‡ªåˆ†ã¨ã®å…±é€šç‚¹**:")
                                st.success(persona_dict.get('common_point', 'è§£æã§ãã¾ã›ã‚“ã§ã—ãŸã€‚'))

                                st.markdown(f"**ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒã‚¤ãƒ³ãƒˆ**:")
                                st.warning(persona_dict.get('communication_point', 'è§£æã§ãã¾ã›ã‚“ã§ã—ãŸã€‚'))

                        # --- 2äººç›®ã®ãƒšãƒ«ã‚½ãƒŠã‚’å³ã‚«ãƒ©ãƒ ã«è¡¨ç¤º (å­˜åœ¨ã™ã‚‹å ´åˆã®ã¿) ---
                        if (i + 1) < num_users:
                            with col2:
                                # 2äººç›®ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ã‚’å–å¾—
                                index2, row2 = user_list[i + 1]
                                target_name2 = row2['ãƒ¦ãƒ¼ã‚¶ãƒ¼']
                                target_scores2 = row2['ã‚¹ã‚³ã‚¢è©³ç´°']
                                target_text2 = row2['å…¨ç™ºè¨€']

                                with st.expander(f"**{target_name2}ã•ã‚“** ã®ãƒšãƒ«ã‚½ãƒŠã‚’è¦‹ã‚‹", expanded=True):
                                    persona_dict = generate_persona_with_retry(target_name2, target_scores2, target_text2, my_scores)
                                    
                                    st.markdown(f"**äººæŸ„**:")
                                    st.info(persona_dict.get('persona', 'è§£æã§ãã¾ã›ã‚“ã§ã—ãŸã€‚'))
                                    
                                    st.markdown(f"**è‡ªåˆ†ã¨ã®å…±é€šç‚¹**:")
                                    st.success(persona_dict.get('common_point', 'è§£æã§ãã¾ã›ã‚“ã§ã—ãŸã€‚'))

                                    st.markdown(f"**ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒã‚¤ãƒ³ãƒˆ**:")
                                    st.warning(persona_dict.get('communication_point', 'è§£æã§ãã¾ã›ã‚“ã§ã—ãŸã€‚'))
            else:
                 st.info("æ€§æ ¼åˆ†æã‚’è¡Œã†ã«ã¯ã€PJãƒ¡ãƒ³ãƒãƒ¼ã®ãƒãƒ£ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã¨è‡ªåˆ†ã®ãƒãƒ£ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸¡æ–¹ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")

    except Exception as e:
        st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")